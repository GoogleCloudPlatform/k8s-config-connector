{# AUTOGENERATED. DO NOT EDIT. #}

{% extends "config-connector/_base.html" %}

{% block page_title %}DataprocWorkflowTemplate{% endblock %}
{% block body %}


<table>
<thead>
<tr>
<th><strong>Property</strong></th>
<th><strong>Value</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>{{gcp_name_short}} Service Name</td>
<td>Dataproc</td>
</tr>
<tr>
<td>{{gcp_name_short}} Service Documentation</td>
<td><a href="/dataproc/docs/">/dataproc/docs/</a></td>
</tr>
<tr>
<td>{{gcp_name_short}} REST Resource Name</td>
<td>v1.projects.locations.workflowTemplates</td>
</tr>
<tr>
<td>{{gcp_name_short}} REST Resource Documentation</td>
<td><a href="/dataproc/docs/reference/rest/v1/projects.locations.workflowTemplates">/dataproc/docs/reference/rest/v1/projects.locations.workflowTemplates</a></td>
</tr>
<tr>
<td>{{product_name_short}} Resource Short Names</td>
<td>gcpdataprocworkflowtemplate<br>gcpdataprocworkflowtemplates<br>dataprocworkflowtemplate</td>
</tr>
<tr>
<td>{{product_name_short}} Service Name</td>
<td>dataproc.googleapis.com</td>
</tr>
<tr>
<td>{{product_name_short}} Resource Fully Qualified Name</td>
<td>dataprocworkflowtemplates.dataproc.cnrm.cloud.google.com</td>
</tr>

<tr>
    <td>Can Be Referenced by IAMPolicy/IAMPolicyMember</td>
    <td>No</td>
</tr>


<tr>
<td>{{product_name_short}} Default Average Reconcile Interval In Seconds</td>
<td>600</td>
</tr>
</tbody>
</table>

## Custom Resource Definition Properties



### Spec
#### Schema
```yaml
dagTimeout: string
encryptionConfig:
  kmsKeyRef:
    external: string
    name: string
    namespace: string
id: string
jobs:
- flinkJob:
    args:
    - string
    jarFileURIs:
    - string
    loggingConfig: {}
    mainClass: string
    mainJarFileURI: string
    properties:
      string: string
    savepointURI: string
  hadoopJob:
    archiveURIs:
    - string
    args:
    - string
    fileURIs:
    - string
    jarFileURIs:
    - string
    loggingConfig: {}
    mainClass: string
    mainJarFileURI: string
    properties:
      string: string
  hiveJob:
    continueOnFailure: boolean
    jarFileURIs:
    - string
    properties:
      string: string
    queryFileURI: string
    queryList:
      queries:
      - string
    scriptVariables:
      string: string
  labels:
    string: string
  pigJob:
    continueOnFailure: boolean
    jarFileURIs:
    - string
    loggingConfig: {}
    properties:
      string: string
    queryFileURI: string
    queryList:
      queries:
      - string
    scriptVariables:
      string: string
  prerequisiteStepIDs:
  - string
  prestoJob:
    clientTags:
    - string
    continueOnFailure: boolean
    loggingConfig: {}
    outputFormat: string
    properties:
      string: string
    queryFileURI: string
    queryList:
      queries:
      - string
  pysparkJob:
    archiveURIs:
    - string
    args:
    - string
    fileURIs:
    - string
    jarFileURIs:
    - string
    loggingConfig: {}
    mainPythonFileURI: string
    properties:
      string: string
    pythonFileURIs:
    - string
  scheduling:
    maxFailuresPerHour: integer
    maxFailuresTotal: integer
  sparkJob:
    archiveURIs:
    - string
    args:
    - string
    fileURIs:
    - string
    jarFileURIs:
    - string
    loggingConfig: {}
    mainClass: string
    mainJarFileURI: string
    properties:
      string: string
  sparkRJob:
    archiveURIs:
    - string
    args:
    - string
    fileURIs:
    - string
    loggingConfig: {}
    mainRFileURI: string
    properties:
      string: string
  sparkSQLJob:
    jarFileURIs:
    - string
    loggingConfig: {}
    properties:
      string: string
    queryFileURI: string
    queryList:
      queries:
      - string
    scriptVariables:
      string: string
  stepID: string
  trinoJob:
    clientTags:
    - string
    continueOnFailure: boolean
    loggingConfig: {}
    outputFormat: string
    properties:
      string: string
    queryFileURI: string
    queryList:
      queries:
      - string
labels:
  string: string
location: string
parameters:
- description: string
  fields:
  - string
  name: string
  validation:
    regex:
      regexes:
      - string
    values:
      values:
      - string
placement:
  clusterSelector:
    clusterLabels:
      string: string
    zone: string
  managedCluster:
    clusterName: string
    config:
      autoscalingConfig:
        policyURI: string
      auxiliaryNodeGroups:
      - nodeGroup:
          labels:
            string: string
          name: string
          nodeGroupConfig:
            accelerators:
            - acceleratorCount: integer
              acceleratorTypeURI: string
            diskConfig:
              bootDiskProvisionedIops: integer
              bootDiskProvisionedThroughput: integer
              bootDiskSizeGB: integer
              bootDiskType: string
              localSsdInterface: string
              numLocalSsds: integer
            imageURI: string
            instanceFlexibilityPolicy:
              instanceSelectionList:
              - machineTypes:
                - string
                rank: integer
              provisioningModelMix:
                standardCapacityBase: integer
                standardCapacityPercentAboveBase: integer
            machineTypeURI: string
            minCPUPlatform: string
            minNumInstances: integer
            numInstances: integer
            preemptibility: string
            startupConfig:
              requiredRegistrationFraction: float
          roles:
          - string
        nodeGroupID: string
      configBucket: string
      dataprocMetricConfig:
        metrics:
        - metricOverrides:
          - string
          metricSource: string
      encryptionConfig:
        gcePDKMSKeyName: string
        kmsKey: string
      endpointConfig:
        enableHTTPPortAccess: boolean
      gceClusterConfig:
        confidentialInstanceConfig:
          enableConfidentialCompute: boolean
        internalIPOnly: boolean
        metadata:
          string: string
        networkURI: string
        nodeGroupAffinity:
          nodeGroupURI: string
        privateIPV6GoogleAccess: string
        reservationAffinity:
          consumeReservationType: string
          key: string
          values:
          - string
        serviceAccount: string
        serviceAccountScopes:
        - string
        shieldedInstanceConfig:
          enableIntegrityMonitoring: boolean
          enableSecureBoot: boolean
          enableVTPM: boolean
        subnetworkURI: string
        tags:
        - string
        zoneURI: string
      initializationActions:
      - executableFile: string
        executionTimeout: string
      lifecycleConfig:
        autoDeleteTTL: string
        autoDeleteTime: string
        idleDeleteTTL: string
      masterConfig:
        accelerators:
        - acceleratorCount: integer
          acceleratorTypeURI: string
        diskConfig:
          bootDiskProvisionedIops: integer
          bootDiskProvisionedThroughput: integer
          bootDiskSizeGB: integer
          bootDiskType: string
          localSsdInterface: string
          numLocalSsds: integer
        imageURI: string
        instanceFlexibilityPolicy:
          instanceSelectionList:
          - machineTypes:
            - string
            rank: integer
          provisioningModelMix:
            standardCapacityBase: integer
            standardCapacityPercentAboveBase: integer
        machineTypeURI: string
        minCPUPlatform: string
        minNumInstances: integer
        numInstances: integer
        preemptibility: string
        startupConfig:
          requiredRegistrationFraction: float
      metastoreConfig:
        dataprocMetastoreService: string
      secondaryWorkerConfig:
        accelerators:
        - acceleratorCount: integer
          acceleratorTypeURI: string
        diskConfig:
          bootDiskProvisionedIops: integer
          bootDiskProvisionedThroughput: integer
          bootDiskSizeGB: integer
          bootDiskType: string
          localSsdInterface: string
          numLocalSsds: integer
        imageURI: string
        instanceFlexibilityPolicy:
          instanceSelectionList:
          - machineTypes:
            - string
            rank: integer
          provisioningModelMix:
            standardCapacityBase: integer
            standardCapacityPercentAboveBase: integer
        machineTypeURI: string
        minCPUPlatform: string
        minNumInstances: integer
        numInstances: integer
        preemptibility: string
        startupConfig:
          requiredRegistrationFraction: float
      securityConfig:
        identityConfig:
          userServiceAccountMapping:
            string: string
        kerberosConfig:
          crossRealmTrustAdminServer: string
          crossRealmTrustKdc: string
          crossRealmTrustRealm: string
          crossRealmTrustSharedPasswordURI: string
          enableKerberos: boolean
          kdcDbKeyURI: string
          keyPasswordURI: string
          keystorePasswordURI: string
          keystoreURI: string
          kmsKeyURI: string
          realm: string
          rootPrincipalPasswordURI: string
          tgtLifetimeHours: integer
          truststorePasswordURI: string
          truststoreURI: string
      softwareConfig:
        imageVersion: string
        optionalComponents:
        - string
        properties:
          string: string
      tempBucket: string
      workerConfig:
        accelerators:
        - acceleratorCount: integer
          acceleratorTypeURI: string
        diskConfig:
          bootDiskProvisionedIops: integer
          bootDiskProvisionedThroughput: integer
          bootDiskSizeGB: integer
          bootDiskType: string
          localSsdInterface: string
          numLocalSsds: integer
        imageURI: string
        instanceFlexibilityPolicy:
          instanceSelectionList:
          - machineTypes:
            - string
            rank: integer
          provisioningModelMix:
            standardCapacityBase: integer
            standardCapacityPercentAboveBase: integer
        machineTypeURI: string
        minCPUPlatform: string
        minNumInstances: integer
        numInstances: integer
        preemptibility: string
        startupConfig:
          requiredRegistrationFraction: float
    labels:
      string: string
projectRef:
  external: string
  kind: string
  name: string
  namespace: string
resourceID: string
version: integer
```

<table class="properties responsive">
<thead>
    <tr>
        <th colspan="2">Fields</th>
    </tr>
</thead>
<tbody>
    <tr>
        <td>
            <p><code>dagTimeout</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. Timeout duration for the DAG of jobs, expressed in seconds (see [JSON representation of duration](https://developers.google.com/protocol-buffers/docs/proto3#json)). The timeout duration must be from 10 minutes ("600s") to 24 hours ("86400s"). The timer begins when the first job is submitted. If the workflow is running at the end of the timeout period, any remaining jobs are cancelled, the workflow is ended, and if the workflow was running on a [managed cluster](/dataproc/docs/concepts/workflows/using-workflows#configuring_or_selecting_a_cluster), the cluster is deleted.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>encryptionConfig</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Encryption settings for encrypting workflow template job arguments.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>encryptionConfig.kmsKeyRef</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. The Cloud KMS key name to use for encrypting
 workflow template job arguments.

 When this this key is provided, the following workflow template
 [job arguments]
 (https://cloud.google.com/dataproc/docs/concepts/workflows/use-workflows#adding_jobs_to_a_template),
 if present, are
 [CMEK
 encrypted](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/customer-managed-encryption#use_cmek_with_workflow_template_data):

 * [FlinkJob
 args](https://cloud.google.com/dataproc/docs/reference/rest/v1/FlinkJob)
 * [HadoopJob
 args](https://cloud.google.com/dataproc/docs/reference/rest/v1/HadoopJob)
 * [SparkJob
 args](https://cloud.google.com/dataproc/docs/reference/rest/v1/SparkJob)
 * [SparkRJob
 args](https://cloud.google.com/dataproc/docs/reference/rest/v1/SparkRJob)
 * [PySparkJob
 args](https://cloud.google.com/dataproc/docs/reference/rest/v1/PySparkJob)
 * [SparkSqlJob](https://cloud.google.com/dataproc/docs/reference/rest/v1/SparkSqlJob)
   scriptVariables and queryList.queries
 * [HiveJob](https://cloud.google.com/dataproc/docs/reference/rest/v1/HiveJob)
   scriptVariables and queryList.queries
 * [PigJob](https://cloud.google.com/dataproc/docs/reference/rest/v1/PigJob)
   scriptVariables and queryList.queries
 * [PrestoJob](https://cloud.google.com/dataproc/docs/reference/rest/v1/PrestoJob)
   scriptVariables and queryList.queries{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>encryptionConfig.kmsKeyRef.external</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}A reference to an externally managed KMSCryptoKey. Should be in the format `projects/[kms_project_id]/locations/[region]/keyRings/[key_ring_id]/cryptoKeys/[key]`.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>encryptionConfig.kmsKeyRef.name</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}The `name` of a `KMSCryptoKey` resource.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>encryptionConfig.kmsKeyRef.namespace</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}The `namespace` of a `KMSCryptoKey` resource.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>id</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (object)</code></p>
            <p>{% verbatim %}Required. The Directed Acyclic Graph of Jobs to submit.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].flinkJob</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Job is a Flink job.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].flinkJob.args</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Optional. The arguments to pass to the driver. Do not include arguments, such as `--conf`, that can be set as job properties, since a collision might occur that causes an incorrect job submission.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].flinkJob.args[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].flinkJob.jarFileURIs</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Optional. HCFS URIs of jar files to add to the CLASSPATHs of the Flink driver and tasks.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].flinkJob.jarFileURIs[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].flinkJob.loggingConfig</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. The runtime log config for job execution.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].flinkJob.mainClass</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}The name of the driver's main class. The jar file that contains the class must be in the default CLASSPATH or specified in [jarFileURIs][google.cloud.dataproc.v1.FlinkJob.jar_file_uris].{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].flinkJob.mainJarFileURI</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}The HCFS URI of the jar file that contains the main class.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].flinkJob.properties</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">map (key: string, value: string)</code></p>
            <p>{% verbatim %}Optional. A mapping of property names to values, used to configure Flink. Properties that conflict with values set by the Dataproc API might be overwritten. Can include properties set in `/etc/flink/conf/flink-defaults.conf` and classes in user code.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].flinkJob.savepointURI</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. HCFS URI of the savepoint, which contains the last saved progress for starting the current job.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].hadoopJob</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Job is a Hadoop job.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].hadoopJob.archiveURIs</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Optional. HCFS URIs of archives to be extracted in the working directory of Hadoop drivers and tasks. Supported file types: .jar, .tar, .tar.gz, .tgz, or .zip.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].hadoopJob.archiveURIs[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].hadoopJob.args</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Optional. The arguments to pass to the driver. Do not include arguments, such as `-libjars` or `-Dfoo=bar`, that can be set as job properties, since a collision might occur that causes an incorrect job submission.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].hadoopJob.args[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].hadoopJob.fileURIs</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Optional. HCFS (Hadoop Compatible Filesystem) URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].hadoopJob.fileURIs[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].hadoopJob.jarFileURIs</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Optional. Jar file URIs to add to the CLASSPATHs of the Hadoop driver and tasks.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].hadoopJob.jarFileURIs[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].hadoopJob.loggingConfig</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. The runtime log config for job execution.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].hadoopJob.mainClass</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in `jar_file_uris`.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].hadoopJob.mainJarFileURI</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].hadoopJob.properties</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">map (key: string, value: string)</code></p>
            <p>{% verbatim %}Optional. A mapping of property names to values, used to configure Hadoop. Properties that conflict with values set by the Dataproc API might be overwritten. Can include properties set in `/etc/hadoop/conf/*-site` and classes in user code.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].hiveJob</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Job is a Hive job.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].hiveJob.continueOnFailure</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">boolean</code></p>
            <p>{% verbatim %}Optional. Whether to continue executing queries if a query fails. The default value is `false`. Setting to `true` can be useful when executing independent parallel queries.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].hiveJob.jarFileURIs</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Optional. HCFS URIs of jar files to add to the CLASSPATH of the Hive server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes and UDFs.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].hiveJob.jarFileURIs[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].hiveJob.properties</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">map (key: string, value: string)</code></p>
            <p>{% verbatim %}Optional. A mapping of property names and values, used to configure Hive. Properties that conflict with values set by the Dataproc API might be overwritten. Can include properties set in `/etc/hadoop/conf/*-site.xml`, /etc/hive/conf/hive-site.xml, and classes in user code.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].hiveJob.queryFileURI</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}The HCFS URI of the script that contains Hive queries.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].hiveJob.queryList</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}A list of queries.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].hiveJob.queryList.queries</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Required. The queries to execute. You do not need to end a query expression
 with a semicolon. Multiple queries can be specified in one
 string by separating each with a semicolon. Here is an example of a
 Dataproc API snippet that uses a QueryList to specify a HiveJob:

     "hiveJob": {
       "queryList": {
         "queries": [
           "query1",
           "query2",
           "query3;query4",
         ]
       }
     }{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].hiveJob.queryList.queries[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].hiveJob.scriptVariables</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">map (key: string, value: string)</code></p>
            <p>{% verbatim %}Optional. Mapping of query variable names to values (equivalent to the Hive command: `SET name="value";`).{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].labels</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">map (key: string, value: string)</code></p>
            <p>{% verbatim %}Optional. The labels to associate with this job.

 Label keys must be between 1 and 63 characters long, and must conform to
 the following regular expression:
 [\p{Ll}\p{Lo}][\p{Ll}\p{Lo}\p{N}_-]{0,62}

 Label values must be between 1 and 63 characters long, and must conform to
 the following regular expression: [\p{Ll}\p{Lo}\p{N}_-]{0,63}

 No more than 32 labels can be associated with a given job.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].pigJob</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Job is a Pig job.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].pigJob.continueOnFailure</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">boolean</code></p>
            <p>{% verbatim %}Optional. Whether to continue executing queries if a query fails. The default value is `false`. Setting to `true` can be useful when executing independent parallel queries.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].pigJob.jarFileURIs</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Optional. HCFS URIs of jar files to add to the CLASSPATH of the Pig Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].pigJob.jarFileURIs[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].pigJob.loggingConfig</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. The runtime log config for job execution.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].pigJob.properties</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">map (key: string, value: string)</code></p>
            <p>{% verbatim %}Optional. A mapping of property names to values, used to configure Pig. Properties that conflict with values set by the Dataproc API might be overwritten. Can include properties set in `/etc/hadoop/conf/*-site.xml`, /etc/pig/conf/pig.properties, and classes in user code.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].pigJob.queryFileURI</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}The HCFS URI of the script that contains the Pig queries.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].pigJob.queryList</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}A list of queries.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].pigJob.queryList.queries</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Required. The queries to execute. You do not need to end a query expression
 with a semicolon. Multiple queries can be specified in one
 string by separating each with a semicolon. Here is an example of a
 Dataproc API snippet that uses a QueryList to specify a HiveJob:

     "hiveJob": {
       "queryList": {
         "queries": [
           "query1",
           "query2",
           "query3;query4",
         ]
       }
     }{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].pigJob.queryList.queries[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].pigJob.scriptVariables</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">map (key: string, value: string)</code></p>
            <p>{% verbatim %}Optional. Mapping of query variable names to values (equivalent to the Pig command: `name=[value]`).{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].prerequisiteStepIDs</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Optional. The optional list of prerequisite job step_ids. If not specified, the job will start at the beginning of workflow.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].prerequisiteStepIDs[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].prestoJob</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Job is a Presto job.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].prestoJob.clientTags</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Optional. Presto client tags to attach to this query{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].prestoJob.clientTags[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].prestoJob.continueOnFailure</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">boolean</code></p>
            <p>{% verbatim %}Optional. Whether to continue executing queries if a query fails. The default value is `false`. Setting to `true` can be useful when executing independent parallel queries.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].prestoJob.loggingConfig</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. The runtime log config for job execution.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].prestoJob.outputFormat</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. The format in which query output will be displayed. See the Presto documentation for supported output formats{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].prestoJob.properties</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">map (key: string, value: string)</code></p>
            <p>{% verbatim %}Optional. A mapping of property names to values. Used to set Presto [session properties](https://prestodb.io/docs/current/sql/set-session.html) Equivalent to using the --session flag in the Presto CLI{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].prestoJob.queryFileURI</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}The HCFS URI of the script that contains SQL queries.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].prestoJob.queryList</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}A list of queries.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].prestoJob.queryList.queries</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Required. The queries to execute. You do not need to end a query expression
 with a semicolon. Multiple queries can be specified in one
 string by separating each with a semicolon. Here is an example of a
 Dataproc API snippet that uses a QueryList to specify a HiveJob:

     "hiveJob": {
       "queryList": {
         "queries": [
           "query1",
           "query2",
           "query3;query4",
         ]
       }
     }{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].prestoJob.queryList.queries[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].pysparkJob</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Job is a PySpark job.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].pysparkJob.archiveURIs</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].pysparkJob.archiveURIs[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].pysparkJob.args</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Optional. The arguments to pass to the driver.  Do not include arguments, such as `--conf`, that can be set as job properties, since a collision may occur that causes an incorrect job submission.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].pysparkJob.args[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].pysparkJob.fileURIs</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].pysparkJob.fileURIs[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].pysparkJob.jarFileURIs</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Optional. HCFS URIs of jar files to add to the CLASSPATHs of the Python driver and tasks.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].pysparkJob.jarFileURIs[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].pysparkJob.loggingConfig</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. The runtime log config for job execution.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].pysparkJob.mainPythonFileURI</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Required. The HCFS URI of the main Python file to use as the driver. Must be a .py file.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].pysparkJob.properties</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">map (key: string, value: string)</code></p>
            <p>{% verbatim %}Optional. A mapping of property names to values, used to configure PySpark. Properties that conflict with values set by the Dataproc API might be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].pysparkJob.pythonFileURIs</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Optional. HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].pysparkJob.pythonFileURIs[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].scheduling</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Job scheduling configuration.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].scheduling.maxFailuresPerHour</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Optional. Maximum number of times per hour a driver can be restarted as
 a result of driver exiting with non-zero code before job is
 reported failed.

 A job might be reported as thrashing if the driver exits with a non-zero
 code four times within a 10-minute window.

 Maximum value is 10.

 **Note:** This restartable job option is not supported in Dataproc
 [workflow templates]
 (https://cloud.google.com/dataproc/docs/concepts/workflows/using-workflows#adding_jobs_to_a_template).{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].scheduling.maxFailuresTotal</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Optional. Maximum total number of times a driver can be restarted as a
 result of the driver exiting with a non-zero code. After the maximum number
 is reached, the job will be reported as failed.

 Maximum value is 240.

 **Note:** Currently, this restartable job option is
 not supported in Dataproc
 [workflow
 templates](https://cloud.google.com/dataproc/docs/concepts/workflows/using-workflows#adding_jobs_to_a_template).{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].sparkJob</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Job is a Spark job.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].sparkJob.archiveURIs</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].sparkJob.archiveURIs[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].sparkJob.args</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Optional. The arguments to pass to the driver. Do not include arguments, such as `--conf`, that can be set as job properties, since a collision may occur that causes an incorrect job submission.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].sparkJob.args[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].sparkJob.fileURIs</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].sparkJob.fileURIs[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].sparkJob.jarFileURIs</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Optional. HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].sparkJob.jarFileURIs[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].sparkJob.loggingConfig</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. The runtime log config for job execution.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].sparkJob.mainClass</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}The name of the driver's main class. The jar file that contains the class must be in the default CLASSPATH or specified in SparkJob.jar_file_uris.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].sparkJob.mainJarFileURI</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}The HCFS URI of the jar file that contains the main class.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].sparkJob.properties</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">map (key: string, value: string)</code></p>
            <p>{% verbatim %}Optional. A mapping of property names to values, used to configure Spark. Properties that conflict with values set by the Dataproc API might be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].sparkRJob</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Job is a SparkR job.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].sparkRJob.archiveURIs</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].sparkRJob.archiveURIs[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].sparkRJob.args</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Optional. The arguments to pass to the driver.  Do not include arguments, such as `--conf`, that can be set as job properties, since a collision may occur that causes an incorrect job submission.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].sparkRJob.args[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].sparkRJob.fileURIs</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].sparkRJob.fileURIs[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].sparkRJob.loggingConfig</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. The runtime log config for job execution.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].sparkRJob.mainRFileURI</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Required. The HCFS URI of the main R file to use as the driver. Must be a .R file.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].sparkRJob.properties</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">map (key: string, value: string)</code></p>
            <p>{% verbatim %}Optional. A mapping of property names to values, used to configure SparkR. Properties that conflict with values set by the Dataproc API might be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].sparkSQLJob</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Job is a SparkSql job.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].sparkSQLJob.jarFileURIs</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].sparkSQLJob.jarFileURIs[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].sparkSQLJob.loggingConfig</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. The runtime log config for job execution.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].sparkSQLJob.properties</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">map (key: string, value: string)</code></p>
            <p>{% verbatim %}Optional. A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API might be overwritten.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].sparkSQLJob.queryFileURI</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}The HCFS URI of the script that contains SQL queries.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].sparkSQLJob.queryList</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}A list of queries.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].sparkSQLJob.queryList.queries</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Required. The queries to execute. You do not need to end a query expression
 with a semicolon. Multiple queries can be specified in one
 string by separating each with a semicolon. Here is an example of a
 Dataproc API snippet that uses a QueryList to specify a HiveJob:

     "hiveJob": {
       "queryList": {
         "queries": [
           "query1",
           "query2",
           "query3;query4",
         ]
       }
     }{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].sparkSQLJob.queryList.queries[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].sparkSQLJob.scriptVariables</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">map (key: string, value: string)</code></p>
            <p>{% verbatim %}Optional. Mapping of query variable names to values (equivalent to the Spark SQL command: SET `name="value";`).{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].stepID</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Required. The step id. The id must be unique among all jobs
 within the template.

 The step id is used as prefix for job id, as job
 `goog-dataproc-workflow-step-id` label, and in
 [prerequisiteStepIds][google.cloud.dataproc.v1.OrderedJob.prerequisite_step_ids]
 field from other steps.

 The id must contain only letters (a-z, A-Z), numbers (0-9),
 underscores (_), and hyphens (-). Cannot begin or end with underscore
 or hyphen. Must consist of between 3 and 50 characters.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].trinoJob</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Job is a Trino job.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].trinoJob.clientTags</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Optional. Trino client tags to attach to this query{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].trinoJob.clientTags[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].trinoJob.continueOnFailure</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">boolean</code></p>
            <p>{% verbatim %}Optional. Whether to continue executing queries if a query fails. The default value is `false`. Setting to `true` can be useful when executing independent parallel queries.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].trinoJob.loggingConfig</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. The runtime log config for job execution.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].trinoJob.outputFormat</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. The format in which query output will be displayed. See the Trino documentation for supported output formats{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].trinoJob.properties</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">map (key: string, value: string)</code></p>
            <p>{% verbatim %}Optional. A mapping of property names to values. Used to set Trino [session properties](https://trino.io/docs/current/sql/set-session.html) Equivalent to using the --session flag in the Trino CLI{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].trinoJob.queryFileURI</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}The HCFS URI of the script that contains SQL queries.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].trinoJob.queryList</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}A list of queries.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].trinoJob.queryList.queries</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Required. The queries to execute. You do not need to end a query expression
 with a semicolon. Multiple queries can be specified in one
 string by separating each with a semicolon. Here is an example of a
 Dataproc API snippet that uses a QueryList to specify a HiveJob:

     "hiveJob": {
       "queryList": {
         "queries": [
           "query1",
           "query2",
           "query3;query4",
         ]
       }
     }{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>jobs[].trinoJob.queryList.queries[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>labels</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">map (key: string, value: string)</code></p>
            <p>{% verbatim %}Optional. The labels to associate with this template. These labels
 will be propagated to all jobs and clusters created by the workflow
 instance.

 Label **keys** must contain 1 to 63 characters, and must conform to
 [RFC 1035](https://www.ietf.org/rfc/rfc1035.txt).

 Label **values** may be empty, but, if present, must contain 1 to 63
 characters, and must conform to
 [RFC 1035](https://www.ietf.org/rfc/rfc1035.txt).

 No more than 32 labels can be associated with a template.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>location</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Required.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>parameters</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (object)</code></p>
            <p>{% verbatim %}Optional. Template parameters whose values are substituted into the template. Values for parameters must be provided when the template is instantiated.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>parameters[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>parameters[].description</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. Brief description of the parameter. Must not exceed 1024 characters.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>parameters[].fields</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Required. Paths to all fields that the parameter replaces.
 A field is allowed to appear in at most one parameter's list of field
 paths.

 A field path is similar in syntax to a
 [google.protobuf.FieldMask][google.protobuf.FieldMask]. For example, a
 field path that references the zone field of a workflow template's cluster
 selector would be specified as `placement.clusterSelector.zone`.

 Also, field paths can reference fields using the following syntax:

 * Values in maps can be referenced by key:
     * labels['key']
     * placement.clusterSelector.clusterLabels['key']
     * placement.managedCluster.labels['key']
     * placement.clusterSelector.clusterLabels['key']
     * jobs['step-id'].labels['key']

 * Jobs in the jobs list can be referenced by step-id:
     * jobs['step-id'].hadoopJob.mainJarFileURI
     * jobs['step-id'].hiveJob.queryFileURI
     * jobs['step-id'].pySparkJob.mainPythonFileURI
     * jobs['step-id'].hadoopJob.jarFileURIs[0]
     * jobs['step-id'].hadoopJob.archiveURIs[0]
     * jobs['step-id'].hadoopJob.fileURIs[0]
     * jobs['step-id'].pySparkJob.pythonFileURIs[0]

 * Items in repeated fields can be referenced by a zero-based index:
     * jobs['step-id'].sparkJob.args[0]

 * Other examples:
     * jobs['step-id'].hadoopJob.properties['key']
     * jobs['step-id'].hadoopJob.args[0]
     * jobs['step-id'].hiveJob.scriptVariables['key']
     * jobs['step-id'].hadoopJob.mainJarFileURI
     * placement.clusterSelector.zone

 It may not be possible to parameterize maps and repeated fields in their
 entirety since only individual map values and individual items in repeated
 fields can be referenced. For example, the following field paths are
 invalid:

 - placement.clusterSelector.clusterLabels
 - jobs['step-id'].sparkJob.args{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>parameters[].fields[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>parameters[].name</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Required. Parameter name. The parameter name is used as the key, and paired with the parameter value, which are passed to the template when the template is instantiated. The name must contain only capital letters (A-Z), numbers (0-9), and underscores (_), and must not start with a number. The maximum length is 40 characters.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>parameters[].validation</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Validation rules to be applied to this parameter's value.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>parameters[].validation.regex</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Validation based on regular expressions.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>parameters[].validation.regex.regexes</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Required. RE2 regular expressions used to validate the parameter's value. The value must match the regex in its entirety (substring matches are not sufficient).{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>parameters[].validation.regex.regexes[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>parameters[].validation.values</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Validation based on a list of allowed values.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>parameters[].validation.values.values</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Required. List of allowed values for the parameter.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>parameters[].validation.values.values[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement</code></p>
            <p><i>Required</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Required. WorkflowTemplate scheduling information.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.clusterSelector</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. A selector that chooses target cluster for jobs based
 on metadata.

 The selector is evaluated at the time each job is submitted.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.clusterSelector.clusterLabels</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">map (key: string, value: string)</code></p>
            <p>{% verbatim %}Required. The cluster labels. Cluster must have all labels to match.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.clusterSelector.zone</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. The zone where workflow process executes. This parameter does not
 affect the selection of the cluster.

 If unspecified, the zone of the first cluster matching the selector
 is used.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}A cluster that is managed by the workflow.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.clusterName</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Required. The cluster name prefix. A unique cluster name will be formed by
 appending a random suffix.

 The name must contain only lower-case letters (a-z), numbers (0-9),
 and hyphens (-). Must begin with a letter. Cannot begin or end with
 hyphen. Must consist of between 2 and 35 characters.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Required. The cluster configuration.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.autoscalingConfig</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Autoscaling config for the policy associated with the cluster. Cluster does not autoscale if this field is unset.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.autoscalingConfig.policyURI</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. The autoscaling policy used by the cluster.

 Only resource names including projectid and location (region) are valid.
 Examples:

 * `https://www.googleapis.com/compute/v1/projects/[project_id]/locations/[dataproc_region]/autoscalingPolicies/[policy_id]`
 * `projects/[project_id]/locations/[dataproc_region]/autoscalingPolicies/[policy_id]`

 Note that the policy must be in the same project and Dataproc region.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.auxiliaryNodeGroups</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (object)</code></p>
            <p>{% verbatim %}Optional. The node group settings.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.auxiliaryNodeGroups[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.auxiliaryNodeGroups[].nodeGroup</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Required. Node group configuration.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.auxiliaryNodeGroups[].nodeGroup.labels</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">map (key: string, value: string)</code></p>
            <p>{% verbatim %}Optional. Node group labels.

 * Label **keys** must consist of from 1 to 63 characters and conform to
   [RFC 1035](https://www.ietf.org/rfc/rfc1035.txt).
 * Label **values** can be empty. If specified, they must consist of from
   1 to 63 characters and conform to [RFC 1035]
   (https://www.ietf.org/rfc/rfc1035.txt).
 * The node group must have no more than 32 labels.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.auxiliaryNodeGroups[].nodeGroup.name</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}The Node group [resource name](https://aip.dev/122).{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.auxiliaryNodeGroups[].nodeGroup.nodeGroupConfig</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. The node group instance group configuration.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.auxiliaryNodeGroups[].nodeGroup.nodeGroupConfig.accelerators</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (object)</code></p>
            <p>{% verbatim %}Optional. The Compute Engine accelerator configuration for these instances.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.auxiliaryNodeGroups[].nodeGroup.nodeGroupConfig.accelerators[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.auxiliaryNodeGroups[].nodeGroup.nodeGroupConfig.accelerators[].acceleratorCount</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}The number of the accelerator cards of this type exposed to this instance.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.auxiliaryNodeGroups[].nodeGroup.nodeGroupConfig.accelerators[].acceleratorTypeURI</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Full URL, partial URI, or short name of the accelerator type resource to
 expose to this instance. See
 [Compute Engine
 AcceleratorTypes](https://cloud.google.com/compute/docs/reference/v1/acceleratorTypes).

 Examples:

 * `https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone]/acceleratorTypes/nvidia-tesla-t4`
 * `projects/[project_id]/zones/[zone]/acceleratorTypes/nvidia-tesla-t4`
 * `nvidia-tesla-t4`

 **Auto Zone Exception**: If you are using the Dataproc
 [Auto Zone
 Placement](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement)
 feature, you must use the short name of the accelerator type
 resource, for example, `nvidia-tesla-t4`.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.auxiliaryNodeGroups[].nodeGroup.nodeGroupConfig.diskConfig</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Disk option config settings.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.auxiliaryNodeGroups[].nodeGroup.nodeGroupConfig.diskConfig.bootDiskProvisionedIops</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Optional. Indicates how many IOPS to provision for the disk. This sets the number of I/O operations per second that the disk can handle. Note: This field is only supported if boot_disk_type is hyperdisk-balanced.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.auxiliaryNodeGroups[].nodeGroup.nodeGroupConfig.diskConfig.bootDiskProvisionedThroughput</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Optional. Indicates how much throughput to provision for the disk. This sets the number of throughput mb per second that the disk can handle. Values must be greater than or equal to 1. Note: This field is only supported if boot_disk_type is hyperdisk-balanced.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.auxiliaryNodeGroups[].nodeGroup.nodeGroupConfig.diskConfig.bootDiskSizeGB</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Optional. Size in GB of the boot disk (default is 500GB).{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.auxiliaryNodeGroups[].nodeGroup.nodeGroupConfig.diskConfig.bootDiskType</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. Type of the boot disk (default is "pd-standard"). Valid values: "pd-balanced" (Persistent Disk Balanced Solid State Drive), "pd-ssd" (Persistent Disk Solid State Drive), or "pd-standard" (Persistent Disk Hard Disk Drive). See [Disk types](https://cloud.google.com/compute/docs/disks#disk-types).{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.auxiliaryNodeGroups[].nodeGroup.nodeGroupConfig.diskConfig.localSsdInterface</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. Interface type of local SSDs (default is "scsi"). Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile Memory Express). See [local SSD performance](https://cloud.google.com/compute/docs/disks/local-ssd#performance).{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.auxiliaryNodeGroups[].nodeGroup.nodeGroupConfig.diskConfig.numLocalSsds</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Optional. Number of attached SSDs, from 0 to 8 (default is 0).
 If SSDs are not attached, the boot disk is used to store runtime logs and
 [HDFS](https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data.
 If one or more SSDs are attached, this runtime bulk
 data is spread across them, and the boot disk contains only basic
 config and installed binaries.

 Note: Local SSD options may vary by machine type and number of vCPUs
 selected.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.auxiliaryNodeGroups[].nodeGroup.nodeGroupConfig.imageURI</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. The Compute Engine image resource used for cluster instances.

 The URI can represent an image or image family.

 Image examples:

 * `https://www.googleapis.com/compute/v1/projects/[project_id]/global/images/[image-id]`
 * `projects/[project_id]/global/images/[image-id]`
 * `image-id`

 Image family examples. Dataproc will use the most recent
 image from the family:

 * `https://www.googleapis.com/compute/v1/projects/[project_id]/global/images/family/[custom-image-family-name]`
 * `projects/[project_id]/global/images/family/[custom-image-family-name]`

 If the URI is unspecified, it will be inferred from
 `SoftwareConfig.image_version` or the system default.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.auxiliaryNodeGroups[].nodeGroup.nodeGroupConfig.instanceFlexibilityPolicy</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Instance flexibility Policy allowing a mixture of VM shapes and provisioning models.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.auxiliaryNodeGroups[].nodeGroup.nodeGroupConfig.instanceFlexibilityPolicy.instanceSelectionList</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (object)</code></p>
            <p>{% verbatim %}Optional. List of instance selection options that the group will use when creating new VMs.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.auxiliaryNodeGroups[].nodeGroup.nodeGroupConfig.instanceFlexibilityPolicy.instanceSelectionList[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.auxiliaryNodeGroups[].nodeGroup.nodeGroupConfig.instanceFlexibilityPolicy.instanceSelectionList[].machineTypes</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Optional. Full machine-type names, e.g. "n1-standard-16".{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.auxiliaryNodeGroups[].nodeGroup.nodeGroupConfig.instanceFlexibilityPolicy.instanceSelectionList[].machineTypes[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.auxiliaryNodeGroups[].nodeGroup.nodeGroupConfig.instanceFlexibilityPolicy.instanceSelectionList[].rank</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Optional. Preference of this instance selection. Lower number means higher preference. Dataproc will first try to create a VM based on the machine-type with priority rank and fallback to next rank based on availability. Machine types and instance selections with the same priority have the same preference.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.auxiliaryNodeGroups[].nodeGroup.nodeGroupConfig.instanceFlexibilityPolicy.provisioningModelMix</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Defines how the Group selects the provisioning model to ensure required reliability.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.auxiliaryNodeGroups[].nodeGroup.nodeGroupConfig.instanceFlexibilityPolicy.provisioningModelMix.standardCapacityBase</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Optional. The base capacity that will always use Standard VMs to avoid risk of more preemption than the minimum capacity you need. Dataproc will create only standard VMs until it reaches standard_capacity_base, then it will start using standard_capacity_percent_above_base to mix Spot with Standard VMs. eg. If 15 instances are requested and standard_capacity_base is 5, Dataproc will create 5 standard VMs and then start mixing spot and standard VMs for remaining 10 instances.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.auxiliaryNodeGroups[].nodeGroup.nodeGroupConfig.instanceFlexibilityPolicy.provisioningModelMix.standardCapacityPercentAboveBase</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Optional. The percentage of target capacity that should use Standard VM. The remaining percentage will use Spot VMs. The percentage applies only to the capacity above standard_capacity_base. eg. If 15 instances are requested and standard_capacity_base is 5 and standard_capacity_percent_above_base is 30, Dataproc will create 5 standard VMs and then start mixing spot and standard VMs for remaining 10 instances. The mix will be 30% standard and 70% spot.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.auxiliaryNodeGroups[].nodeGroup.nodeGroupConfig.machineTypeURI</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. The Compute Engine machine type used for cluster instances.

 A full URL, partial URI, or short name are valid. Examples:

 * `https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone]/machineTypes/n1-standard-2`
 * `projects/[project_id]/zones/[zone]/machineTypes/n1-standard-2`
 * `n1-standard-2`

 **Auto Zone Exception**: If you are using the Dataproc
 [Auto Zone
 Placement](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement)
 feature, you must use the short name of the machine type
 resource, for example, `n1-standard-2`.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.auxiliaryNodeGroups[].nodeGroup.nodeGroupConfig.minCPUPlatform</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. Specifies the minimum cpu platform for the Instance Group. See [Dataproc -> Minimum CPU Platform](https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.auxiliaryNodeGroups[].nodeGroup.nodeGroupConfig.minNumInstances</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Optional. The minimum number of primary worker instances to create.
 If `min_num_instances` is set, cluster creation will succeed if
 the number of primary workers created is at least equal to the
 `min_num_instances` number.

 Example: Cluster creation request with `num_instances` = `5` and
 `min_num_instances` = `3`:

 *  If 4 VMs are created and 1 instance fails,
    the failed VM is deleted. The cluster is
    resized to 4 instances and placed in a `RUNNING` state.
 *  If 2 instances are created and 3 instances fail,
    the cluster in placed in an `ERROR` state. The failed VMs
    are not deleted.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.auxiliaryNodeGroups[].nodeGroup.nodeGroupConfig.numInstances</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Optional. The number of VM instances in the instance group. For [HA cluster](/dataproc/docs/concepts/configuring-clusters/high-availability) [master_config](#FIELDS.master_config) groups, **must be set to 3**. For standard cluster [master_config](#FIELDS.master_config) groups, **must be set to 1**.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.auxiliaryNodeGroups[].nodeGroup.nodeGroupConfig.preemptibility</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. Specifies the preemptibility of the instance group.

 The default value for master and worker groups is
 `NON_PREEMPTIBLE`. This default cannot be changed.

 The default value for secondary instances is
 `PREEMPTIBLE`.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.auxiliaryNodeGroups[].nodeGroup.nodeGroupConfig.startupConfig</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Configuration to handle the startup of instances during cluster create and update process.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.auxiliaryNodeGroups[].nodeGroup.nodeGroupConfig.startupConfig.requiredRegistrationFraction</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">float</code></p>
            <p>{% verbatim %}Optional. The config setting to enable cluster creation/ updation to be successful only after required_registration_fraction of instances are up and running. This configuration is applicable to only secondary workers for now. The cluster will fail if required_registration_fraction of instances are not available. This will include instance creation, agent registration, and service registration (if enabled).{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.auxiliaryNodeGroups[].nodeGroup.roles</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Required. Node group roles.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.auxiliaryNodeGroups[].nodeGroup.roles[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.auxiliaryNodeGroups[].nodeGroupID</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. A node group ID. Generated if not specified.

 The ID must contain only letters (a-z, A-Z), numbers (0-9),
 underscores (_), and hyphens (-). Cannot begin or end with underscore
 or hyphen. Must consist of from 3 to 33 characters.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.configBucket</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. A Cloud Storage bucket used to stage job dependencies, config files, and job driver console output. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's staging bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket (see [Dataproc staging and temp buckets](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)). **This field requires a Cloud Storage bucket name, not a `gs://...` URI to a Cloud Storage bucket.**{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.dataprocMetricConfig</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. The config for Dataproc metrics.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.dataprocMetricConfig.metrics</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (object)</code></p>
            <p>{% verbatim %}Required. Metrics sources to enable.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.dataprocMetricConfig.metrics[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.dataprocMetricConfig.metrics[].metricOverrides</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Optional. Specify one or more [Custom metrics]
 (https://cloud.google.com/dataproc/docs/guides/dataproc-metrics#custom_metrics)
 to collect for the metric course (for the `SPARK` metric source (any
 [Spark metric]
 (https://spark.apache.org/docs/latest/monitoring.html#metrics) can be
 specified).

 Provide metrics in the following format:
 <code><var>METRIC_SOURCE</var>:<var>INSTANCE</var>:<var>GROUP</var>:<var>METRIC</var></code>
 Use camelcase as appropriate.

 Examples:

 ```
 yarn:ResourceManager:QueueMetrics:AppsCompleted
 spark:driver:DAGScheduler:job.allJobs
 sparkHistoryServer:JVM:Memory:NonHeapMemoryUsage.committed
 hiveserver2:JVM:Memory:NonHeapMemoryUsage.used
 ```

 Notes:

 * Only the specified overridden metrics are collected for the
   metric source. For example, if one or more `spark:executive` metrics
   are listed as metric overrides, other `SPARK` metrics are not
   collected. The collection of the metrics for other enabled custom
   metric sources is unaffected. For example, if both `SPARK` and `YARN`
   metric sources are enabled, and overrides are provided for Spark
   metrics only, all YARN metrics are collected.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.dataprocMetricConfig.metrics[].metricOverrides[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.dataprocMetricConfig.metrics[].metricSource</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Required. A standard set of metrics is collected unless `metricOverrides` are specified for the metric source (see [Custom metrics] (https://cloud.google.com/dataproc/docs/guides/dataproc-metrics#custom_metrics) for more information).{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.encryptionConfig</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Encryption settings for the cluster.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.encryptionConfig.gcePDKMSKeyName</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. The Cloud KMS key resource name to use for persistent disk encryption for all instances in the cluster. See [Use CMEK with cluster data] (https://cloud.google.com//dataproc/docs/concepts/configuring-clusters/customer-managed-encryption#use_cmek_with_cluster_data) for more information.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.encryptionConfig.kmsKey</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. The Cloud KMS key resource name to use for cluster persistent
 disk and job argument encryption. See [Use CMEK with cluster data]
 (https://cloud.google.com//dataproc/docs/concepts/configuring-clusters/customer-managed-encryption#use_cmek_with_cluster_data)
 for more information.

 When this key resource name is provided, the following job arguments of
 the following job types submitted to the cluster are encrypted using CMEK:

 * [FlinkJob
 args](https://cloud.google.com/dataproc/docs/reference/rest/v1/FlinkJob)
 * [HadoopJob
 args](https://cloud.google.com/dataproc/docs/reference/rest/v1/HadoopJob)
 * [SparkJob
 args](https://cloud.google.com/dataproc/docs/reference/rest/v1/SparkJob)
 * [SparkRJob
 args](https://cloud.google.com/dataproc/docs/reference/rest/v1/SparkRJob)
 * [PySparkJob
 args](https://cloud.google.com/dataproc/docs/reference/rest/v1/PySparkJob)
 * [SparkSqlJob](https://cloud.google.com/dataproc/docs/reference/rest/v1/SparkSqlJob)
   scriptVariables and queryList.queries
 * [HiveJob](https://cloud.google.com/dataproc/docs/reference/rest/v1/HiveJob)
   scriptVariables and queryList.queries
 * [PigJob](https://cloud.google.com/dataproc/docs/reference/rest/v1/PigJob)
   scriptVariables and queryList.queries
 * [PrestoJob](https://cloud.google.com/dataproc/docs/reference/rest/v1/PrestoJob)
   scriptVariables and queryList.queries{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.endpointConfig</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Port/endpoint configuration for this cluster{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.endpointConfig.enableHTTPPortAccess</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">boolean</code></p>
            <p>{% verbatim %}Optional. If true, enable http access to specific ports on the cluster from external sources. Defaults to false.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.gceClusterConfig</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. The shared Compute Engine config settings for all instances in a cluster.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.gceClusterConfig.confidentialInstanceConfig</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Confidential Instance Config for clusters using [Confidential VMs](https://cloud.google.com/compute/confidential-vm/docs).{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.gceClusterConfig.confidentialInstanceConfig.enableConfidentialCompute</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">boolean</code></p>
            <p>{% verbatim %}Optional. Defines whether the instance should have confidential compute enabled.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.gceClusterConfig.internalIPOnly</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">boolean</code></p>
            <p>{% verbatim %}Optional. This setting applies to subnetwork-enabled networks. It is set to
 `true` by default in clusters created with image versions 2.2.x.

 When set to `true`:

 * All cluster VMs have internal IP addresses.
 * [Google Private Access]
 (https://cloud.google.com/vpc/docs/private-google-access)
 must be enabled to access Dataproc and other Google Cloud APIs.
 * Off-cluster dependencies must be configured to be accessible
 without external IP addresses.

 When set to `false`:

 * Cluster VMs are not restricted to internal IP addresses.
 * Ephemeral external IP addresses are assigned to each cluster VM.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.gceClusterConfig.metadata</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">map (key: string, value: string)</code></p>
            <p>{% verbatim %}Optional. The Compute Engine metadata entries to add to all instances (see [Project and instance metadata](https://cloud.google.com/compute/docs/storing-retrieving-metadata#project_and_instance_metadata)).{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.gceClusterConfig.networkURI</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. The Compute Engine network to be used for machine
 communications. Cannot be specified with subnetwork_uri. If neither
 `network_uri` nor `subnetwork_uri` is specified, the "default" network of
 the project is used, if it exists. Cannot be a "Custom Subnet Network" (see
 [Using Subnetworks](https://cloud.google.com/compute/docs/subnetworks) for
 more information).

 A full URL, partial URI, or short name are valid. Examples:

 * `https://www.googleapis.com/compute/v1/projects/[project_id]/global/networks/default`
 * `projects/[project_id]/global/networks/default`
 * `default`{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.gceClusterConfig.nodeGroupAffinity</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Node Group Affinity for sole-tenant clusters.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.gceClusterConfig.nodeGroupAffinity.nodeGroupURI</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Required. The URI of a
 sole-tenant [node group
 resource](https://cloud.google.com/compute/docs/reference/rest/v1/nodeGroups)
 that the cluster will be created on.

 A full URL, partial URI, or node group name are valid. Examples:

 * `https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone]/nodeGroups/node-group-1`
 * `projects/[project_id]/zones/[zone]/nodeGroups/node-group-1`
 * `node-group-1`{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.gceClusterConfig.privateIPV6GoogleAccess</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. The type of IPv6 access for a cluster.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.gceClusterConfig.reservationAffinity</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Reservation Affinity for consuming Zonal reservation.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.gceClusterConfig.reservationAffinity.consumeReservationType</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. Type of reservation to consume{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.gceClusterConfig.reservationAffinity.key</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. Corresponds to the label key of reservation resource.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.gceClusterConfig.reservationAffinity.values</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Optional. Corresponds to the label values of reservation resource.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.gceClusterConfig.reservationAffinity.values[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.gceClusterConfig.serviceAccount</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. The [Dataproc service
 account](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/service-accounts#service_accounts_in_dataproc)
 (also see [VM Data Plane
 identity](https://cloud.google.com/dataproc/docs/concepts/iam/dataproc-principals#vm_service_account_data_plane_identity))
 used by Dataproc cluster VM instances to access Google Cloud Platform
 services.

 If not specified, the
 [Compute Engine default service
 account](https://cloud.google.com/compute/docs/access/service-accounts#default_service_account)
 is used.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.gceClusterConfig.serviceAccountScopes</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Optional. The URIs of service account scopes to be included in
 Compute Engine instances. The following base set of scopes is always
 included:

 * https://www.googleapis.com/auth/cloud.useraccounts.readonly
 * https://www.googleapis.com/auth/devstorage.read_write
 * https://www.googleapis.com/auth/logging.write

 If no scopes are specified, the following defaults are also provided:

 * https://www.googleapis.com/auth/bigquery
 * https://www.googleapis.com/auth/bigtable.admin.table
 * https://www.googleapis.com/auth/bigtable.data
 * https://www.googleapis.com/auth/devstorage.full_control{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.gceClusterConfig.serviceAccountScopes[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.gceClusterConfig.shieldedInstanceConfig</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Shielded Instance Config for clusters using [Compute Engine Shielded VMs](https://cloud.google.com/security/shielded-cloud/shielded-vm).{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.gceClusterConfig.shieldedInstanceConfig.enableIntegrityMonitoring</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">boolean</code></p>
            <p>{% verbatim %}Optional. Defines whether instances have integrity monitoring enabled.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.gceClusterConfig.shieldedInstanceConfig.enableSecureBoot</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">boolean</code></p>
            <p>{% verbatim %}Optional. Defines whether instances have Secure Boot enabled.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.gceClusterConfig.shieldedInstanceConfig.enableVTPM</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">boolean</code></p>
            <p>{% verbatim %}Optional. Defines whether instances have the vTPM enabled.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.gceClusterConfig.subnetworkURI</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. The Compute Engine subnetwork to be used for machine
 communications. Cannot be specified with network_uri.

 A full URL, partial URI, or short name are valid. Examples:

 * `https://www.googleapis.com/compute/v1/projects/[project_id]/regions/[region]/subnetworks/sub0`
 * `projects/[project_id]/regions/[region]/subnetworks/sub0`
 * `sub0`{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.gceClusterConfig.tags</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}The Compute Engine network tags to add to all instances (see [Tagging instances](https://cloud.google.com/vpc/docs/add-remove-network-tags)).{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.gceClusterConfig.tags[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.gceClusterConfig.zoneURI</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. The Compute Engine zone where the Dataproc cluster will be
 located. If omitted, the service will pick a zone in the cluster's Compute
 Engine region. On a get request, zone will always be present.

 A full URL, partial URI, or short name are valid. Examples:

 * `https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone]`
 * `projects/[project_id]/zones/[zone]`
 * `[zone]`{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.initializationActions</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (object)</code></p>
            <p>{% verbatim %}Optional. Commands to execute on each node after config is
 completed. By default, executables are run on master and all worker nodes.
 You can test a node's `role` metadata to run an executable on
 a master or worker node, as shown below using `curl` (you can also use
 `wget`):

     ROLE=$(curl -H Metadata-Flavor:Google
     http://metadata/computeMetadata/v1/instance/attributes/dataproc-role)
     if [[ "${ROLE}" == 'Master' ]]; then
       ... master specific actions ...
     else
       ... worker specific actions ...
     fi{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.initializationActions[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.initializationActions[].executableFile</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Required. Cloud Storage URI of executable file.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.initializationActions[].executionTimeout</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. Amount of time executable has to complete. Default is
 10 minutes (see JSON representation of
 [Duration](https://developers.google.com/protocol-buffers/docs/proto3#json)).

 Cluster creation fails with an explanatory error message (the
 name of the executable that caused the error and the exceeded timeout
 period) if the executable is not completed at end of the timeout period.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.lifecycleConfig</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Lifecycle setting for the cluster.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.lifecycleConfig.autoDeleteTTL</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. The lifetime duration of cluster. The cluster will be auto-deleted at the end of this period. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of [Duration](https://developers.google.com/protocol-buffers/docs/proto3#json)).{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.lifecycleConfig.autoDeleteTime</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. The time when cluster will be auto-deleted (see JSON representation of [Timestamp](https://developers.google.com/protocol-buffers/docs/proto3#json)).{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.lifecycleConfig.idleDeleteTTL</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. The duration to keep the cluster alive while idling (when no jobs are running). Passing this threshold will cause the cluster to be deleted. Minimum value is 5 minutes; maximum value is 14 days (see JSON representation of [Duration](https://developers.google.com/protocol-buffers/docs/proto3#json)).{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.masterConfig</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. The Compute Engine config settings for the cluster's master instance.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.masterConfig.accelerators</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (object)</code></p>
            <p>{% verbatim %}Optional. The Compute Engine accelerator configuration for these instances.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.masterConfig.accelerators[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.masterConfig.accelerators[].acceleratorCount</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}The number of the accelerator cards of this type exposed to this instance.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.masterConfig.accelerators[].acceleratorTypeURI</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Full URL, partial URI, or short name of the accelerator type resource to
 expose to this instance. See
 [Compute Engine
 AcceleratorTypes](https://cloud.google.com/compute/docs/reference/v1/acceleratorTypes).

 Examples:

 * `https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone]/acceleratorTypes/nvidia-tesla-t4`
 * `projects/[project_id]/zones/[zone]/acceleratorTypes/nvidia-tesla-t4`
 * `nvidia-tesla-t4`

 **Auto Zone Exception**: If you are using the Dataproc
 [Auto Zone
 Placement](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement)
 feature, you must use the short name of the accelerator type
 resource, for example, `nvidia-tesla-t4`.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.masterConfig.diskConfig</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Disk option config settings.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.masterConfig.diskConfig.bootDiskProvisionedIops</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Optional. Indicates how many IOPS to provision for the disk. This sets the number of I/O operations per second that the disk can handle. Note: This field is only supported if boot_disk_type is hyperdisk-balanced.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.masterConfig.diskConfig.bootDiskProvisionedThroughput</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Optional. Indicates how much throughput to provision for the disk. This sets the number of throughput mb per second that the disk can handle. Values must be greater than or equal to 1. Note: This field is only supported if boot_disk_type is hyperdisk-balanced.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.masterConfig.diskConfig.bootDiskSizeGB</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Optional. Size in GB of the boot disk (default is 500GB).{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.masterConfig.diskConfig.bootDiskType</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. Type of the boot disk (default is "pd-standard"). Valid values: "pd-balanced" (Persistent Disk Balanced Solid State Drive), "pd-ssd" (Persistent Disk Solid State Drive), or "pd-standard" (Persistent Disk Hard Disk Drive). See [Disk types](https://cloud.google.com/compute/docs/disks#disk-types).{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.masterConfig.diskConfig.localSsdInterface</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. Interface type of local SSDs (default is "scsi"). Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile Memory Express). See [local SSD performance](https://cloud.google.com/compute/docs/disks/local-ssd#performance).{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.masterConfig.diskConfig.numLocalSsds</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Optional. Number of attached SSDs, from 0 to 8 (default is 0).
 If SSDs are not attached, the boot disk is used to store runtime logs and
 [HDFS](https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data.
 If one or more SSDs are attached, this runtime bulk
 data is spread across them, and the boot disk contains only basic
 config and installed binaries.

 Note: Local SSD options may vary by machine type and number of vCPUs
 selected.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.masterConfig.imageURI</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. The Compute Engine image resource used for cluster instances.

 The URI can represent an image or image family.

 Image examples:

 * `https://www.googleapis.com/compute/v1/projects/[project_id]/global/images/[image-id]`
 * `projects/[project_id]/global/images/[image-id]`
 * `image-id`

 Image family examples. Dataproc will use the most recent
 image from the family:

 * `https://www.googleapis.com/compute/v1/projects/[project_id]/global/images/family/[custom-image-family-name]`
 * `projects/[project_id]/global/images/family/[custom-image-family-name]`

 If the URI is unspecified, it will be inferred from
 `SoftwareConfig.image_version` or the system default.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.masterConfig.instanceFlexibilityPolicy</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Instance flexibility Policy allowing a mixture of VM shapes and provisioning models.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.masterConfig.instanceFlexibilityPolicy.instanceSelectionList</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (object)</code></p>
            <p>{% verbatim %}Optional. List of instance selection options that the group will use when creating new VMs.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.masterConfig.instanceFlexibilityPolicy.instanceSelectionList[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.masterConfig.instanceFlexibilityPolicy.instanceSelectionList[].machineTypes</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Optional. Full machine-type names, e.g. "n1-standard-16".{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.masterConfig.instanceFlexibilityPolicy.instanceSelectionList[].machineTypes[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.masterConfig.instanceFlexibilityPolicy.instanceSelectionList[].rank</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Optional. Preference of this instance selection. Lower number means higher preference. Dataproc will first try to create a VM based on the machine-type with priority rank and fallback to next rank based on availability. Machine types and instance selections with the same priority have the same preference.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.masterConfig.instanceFlexibilityPolicy.provisioningModelMix</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Defines how the Group selects the provisioning model to ensure required reliability.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.masterConfig.instanceFlexibilityPolicy.provisioningModelMix.standardCapacityBase</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Optional. The base capacity that will always use Standard VMs to avoid risk of more preemption than the minimum capacity you need. Dataproc will create only standard VMs until it reaches standard_capacity_base, then it will start using standard_capacity_percent_above_base to mix Spot with Standard VMs. eg. If 15 instances are requested and standard_capacity_base is 5, Dataproc will create 5 standard VMs and then start mixing spot and standard VMs for remaining 10 instances.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.masterConfig.instanceFlexibilityPolicy.provisioningModelMix.standardCapacityPercentAboveBase</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Optional. The percentage of target capacity that should use Standard VM. The remaining percentage will use Spot VMs. The percentage applies only to the capacity above standard_capacity_base. eg. If 15 instances are requested and standard_capacity_base is 5 and standard_capacity_percent_above_base is 30, Dataproc will create 5 standard VMs and then start mixing spot and standard VMs for remaining 10 instances. The mix will be 30% standard and 70% spot.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.masterConfig.machineTypeURI</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. The Compute Engine machine type used for cluster instances.

 A full URL, partial URI, or short name are valid. Examples:

 * `https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone]/machineTypes/n1-standard-2`
 * `projects/[project_id]/zones/[zone]/machineTypes/n1-standard-2`
 * `n1-standard-2`

 **Auto Zone Exception**: If you are using the Dataproc
 [Auto Zone
 Placement](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement)
 feature, you must use the short name of the machine type
 resource, for example, `n1-standard-2`.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.masterConfig.minCPUPlatform</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. Specifies the minimum cpu platform for the Instance Group. See [Dataproc -> Minimum CPU Platform](https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.masterConfig.minNumInstances</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Optional. The minimum number of primary worker instances to create.
 If `min_num_instances` is set, cluster creation will succeed if
 the number of primary workers created is at least equal to the
 `min_num_instances` number.

 Example: Cluster creation request with `num_instances` = `5` and
 `min_num_instances` = `3`:

 *  If 4 VMs are created and 1 instance fails,
    the failed VM is deleted. The cluster is
    resized to 4 instances and placed in a `RUNNING` state.
 *  If 2 instances are created and 3 instances fail,
    the cluster in placed in an `ERROR` state. The failed VMs
    are not deleted.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.masterConfig.numInstances</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Optional. The number of VM instances in the instance group. For [HA cluster](/dataproc/docs/concepts/configuring-clusters/high-availability) [master_config](#FIELDS.master_config) groups, **must be set to 3**. For standard cluster [master_config](#FIELDS.master_config) groups, **must be set to 1**.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.masterConfig.preemptibility</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. Specifies the preemptibility of the instance group.

 The default value for master and worker groups is
 `NON_PREEMPTIBLE`. This default cannot be changed.

 The default value for secondary instances is
 `PREEMPTIBLE`.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.masterConfig.startupConfig</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Configuration to handle the startup of instances during cluster create and update process.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.masterConfig.startupConfig.requiredRegistrationFraction</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">float</code></p>
            <p>{% verbatim %}Optional. The config setting to enable cluster creation/ updation to be successful only after required_registration_fraction of instances are up and running. This configuration is applicable to only secondary workers for now. The cluster will fail if required_registration_fraction of instances are not available. This will include instance creation, agent registration, and service registration (if enabled).{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.metastoreConfig</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Metastore configuration.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.metastoreConfig.dataprocMetastoreService</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Required. Resource name of an existing Dataproc Metastore service.

 Example:

 * `projects/[project_id]/locations/[dataproc_region]/services/[service-name]`{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.secondaryWorkerConfig</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. The Compute Engine config settings for a cluster's secondary worker instances{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.secondaryWorkerConfig.accelerators</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (object)</code></p>
            <p>{% verbatim %}Optional. The Compute Engine accelerator configuration for these instances.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.secondaryWorkerConfig.accelerators[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.secondaryWorkerConfig.accelerators[].acceleratorCount</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}The number of the accelerator cards of this type exposed to this instance.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.secondaryWorkerConfig.accelerators[].acceleratorTypeURI</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Full URL, partial URI, or short name of the accelerator type resource to
 expose to this instance. See
 [Compute Engine
 AcceleratorTypes](https://cloud.google.com/compute/docs/reference/v1/acceleratorTypes).

 Examples:

 * `https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone]/acceleratorTypes/nvidia-tesla-t4`
 * `projects/[project_id]/zones/[zone]/acceleratorTypes/nvidia-tesla-t4`
 * `nvidia-tesla-t4`

 **Auto Zone Exception**: If you are using the Dataproc
 [Auto Zone
 Placement](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement)
 feature, you must use the short name of the accelerator type
 resource, for example, `nvidia-tesla-t4`.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.secondaryWorkerConfig.diskConfig</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Disk option config settings.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.secondaryWorkerConfig.diskConfig.bootDiskProvisionedIops</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Optional. Indicates how many IOPS to provision for the disk. This sets the number of I/O operations per second that the disk can handle. Note: This field is only supported if boot_disk_type is hyperdisk-balanced.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.secondaryWorkerConfig.diskConfig.bootDiskProvisionedThroughput</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Optional. Indicates how much throughput to provision for the disk. This sets the number of throughput mb per second that the disk can handle. Values must be greater than or equal to 1. Note: This field is only supported if boot_disk_type is hyperdisk-balanced.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.secondaryWorkerConfig.diskConfig.bootDiskSizeGB</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Optional. Size in GB of the boot disk (default is 500GB).{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.secondaryWorkerConfig.diskConfig.bootDiskType</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. Type of the boot disk (default is "pd-standard"). Valid values: "pd-balanced" (Persistent Disk Balanced Solid State Drive), "pd-ssd" (Persistent Disk Solid State Drive), or "pd-standard" (Persistent Disk Hard Disk Drive). See [Disk types](https://cloud.google.com/compute/docs/disks#disk-types).{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.secondaryWorkerConfig.diskConfig.localSsdInterface</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. Interface type of local SSDs (default is "scsi"). Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile Memory Express). See [local SSD performance](https://cloud.google.com/compute/docs/disks/local-ssd#performance).{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.secondaryWorkerConfig.diskConfig.numLocalSsds</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Optional. Number of attached SSDs, from 0 to 8 (default is 0).
 If SSDs are not attached, the boot disk is used to store runtime logs and
 [HDFS](https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data.
 If one or more SSDs are attached, this runtime bulk
 data is spread across them, and the boot disk contains only basic
 config and installed binaries.

 Note: Local SSD options may vary by machine type and number of vCPUs
 selected.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.secondaryWorkerConfig.imageURI</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. The Compute Engine image resource used for cluster instances.

 The URI can represent an image or image family.

 Image examples:

 * `https://www.googleapis.com/compute/v1/projects/[project_id]/global/images/[image-id]`
 * `projects/[project_id]/global/images/[image-id]`
 * `image-id`

 Image family examples. Dataproc will use the most recent
 image from the family:

 * `https://www.googleapis.com/compute/v1/projects/[project_id]/global/images/family/[custom-image-family-name]`
 * `projects/[project_id]/global/images/family/[custom-image-family-name]`

 If the URI is unspecified, it will be inferred from
 `SoftwareConfig.image_version` or the system default.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.secondaryWorkerConfig.instanceFlexibilityPolicy</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Instance flexibility Policy allowing a mixture of VM shapes and provisioning models.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.secondaryWorkerConfig.instanceFlexibilityPolicy.instanceSelectionList</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (object)</code></p>
            <p>{% verbatim %}Optional. List of instance selection options that the group will use when creating new VMs.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.secondaryWorkerConfig.instanceFlexibilityPolicy.instanceSelectionList[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.secondaryWorkerConfig.instanceFlexibilityPolicy.instanceSelectionList[].machineTypes</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Optional. Full machine-type names, e.g. "n1-standard-16".{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.secondaryWorkerConfig.instanceFlexibilityPolicy.instanceSelectionList[].machineTypes[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.secondaryWorkerConfig.instanceFlexibilityPolicy.instanceSelectionList[].rank</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Optional. Preference of this instance selection. Lower number means higher preference. Dataproc will first try to create a VM based on the machine-type with priority rank and fallback to next rank based on availability. Machine types and instance selections with the same priority have the same preference.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.secondaryWorkerConfig.instanceFlexibilityPolicy.provisioningModelMix</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Defines how the Group selects the provisioning model to ensure required reliability.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.secondaryWorkerConfig.instanceFlexibilityPolicy.provisioningModelMix.standardCapacityBase</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Optional. The base capacity that will always use Standard VMs to avoid risk of more preemption than the minimum capacity you need. Dataproc will create only standard VMs until it reaches standard_capacity_base, then it will start using standard_capacity_percent_above_base to mix Spot with Standard VMs. eg. If 15 instances are requested and standard_capacity_base is 5, Dataproc will create 5 standard VMs and then start mixing spot and standard VMs for remaining 10 instances.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.secondaryWorkerConfig.instanceFlexibilityPolicy.provisioningModelMix.standardCapacityPercentAboveBase</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Optional. The percentage of target capacity that should use Standard VM. The remaining percentage will use Spot VMs. The percentage applies only to the capacity above standard_capacity_base. eg. If 15 instances are requested and standard_capacity_base is 5 and standard_capacity_percent_above_base is 30, Dataproc will create 5 standard VMs and then start mixing spot and standard VMs for remaining 10 instances. The mix will be 30% standard and 70% spot.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.secondaryWorkerConfig.machineTypeURI</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. The Compute Engine machine type used for cluster instances.

 A full URL, partial URI, or short name are valid. Examples:

 * `https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone]/machineTypes/n1-standard-2`
 * `projects/[project_id]/zones/[zone]/machineTypes/n1-standard-2`
 * `n1-standard-2`

 **Auto Zone Exception**: If you are using the Dataproc
 [Auto Zone
 Placement](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement)
 feature, you must use the short name of the machine type
 resource, for example, `n1-standard-2`.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.secondaryWorkerConfig.minCPUPlatform</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. Specifies the minimum cpu platform for the Instance Group. See [Dataproc -> Minimum CPU Platform](https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.secondaryWorkerConfig.minNumInstances</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Optional. The minimum number of primary worker instances to create.
 If `min_num_instances` is set, cluster creation will succeed if
 the number of primary workers created is at least equal to the
 `min_num_instances` number.

 Example: Cluster creation request with `num_instances` = `5` and
 `min_num_instances` = `3`:

 *  If 4 VMs are created and 1 instance fails,
    the failed VM is deleted. The cluster is
    resized to 4 instances and placed in a `RUNNING` state.
 *  If 2 instances are created and 3 instances fail,
    the cluster in placed in an `ERROR` state. The failed VMs
    are not deleted.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.secondaryWorkerConfig.numInstances</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Optional. The number of VM instances in the instance group. For [HA cluster](/dataproc/docs/concepts/configuring-clusters/high-availability) [master_config](#FIELDS.master_config) groups, **must be set to 3**. For standard cluster [master_config](#FIELDS.master_config) groups, **must be set to 1**.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.secondaryWorkerConfig.preemptibility</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. Specifies the preemptibility of the instance group.

 The default value for master and worker groups is
 `NON_PREEMPTIBLE`. This default cannot be changed.

 The default value for secondary instances is
 `PREEMPTIBLE`.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.secondaryWorkerConfig.startupConfig</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Configuration to handle the startup of instances during cluster create and update process.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.secondaryWorkerConfig.startupConfig.requiredRegistrationFraction</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">float</code></p>
            <p>{% verbatim %}Optional. The config setting to enable cluster creation/ updation to be successful only after required_registration_fraction of instances are up and running. This configuration is applicable to only secondary workers for now. The cluster will fail if required_registration_fraction of instances are not available. This will include instance creation, agent registration, and service registration (if enabled).{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.securityConfig</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Security settings for the cluster.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.securityConfig.identityConfig</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Identity related configuration, including service account based secure multi-tenancy user mappings.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.securityConfig.identityConfig.userServiceAccountMapping</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">map (key: string, value: string)</code></p>
            <p>{% verbatim %}Required. Map of user to service account.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.securityConfig.kerberosConfig</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Kerberos related configuration.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.securityConfig.kerberosConfig.crossRealmTrustAdminServer</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. The admin server (IP or hostname) for the remote trusted realm in a cross realm trust relationship.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.securityConfig.kerberosConfig.crossRealmTrustKdc</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. The KDC (IP or hostname) for the remote trusted realm in a cross realm trust relationship.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.securityConfig.kerberosConfig.crossRealmTrustRealm</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. The remote realm the Dataproc on-cluster KDC will trust, should the user enable cross realm trust.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.securityConfig.kerberosConfig.crossRealmTrustSharedPasswordURI</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. The Cloud Storage URI of a KMS encrypted file containing the shared password between the on-cluster Kerberos realm and the remote trusted realm, in a cross realm trust relationship.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.securityConfig.kerberosConfig.enableKerberos</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">boolean</code></p>
            <p>{% verbatim %}Optional. Flag to indicate whether to Kerberize the cluster (default: false). Set this field to true to enable Kerberos on a cluster.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.securityConfig.kerberosConfig.kdcDbKeyURI</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. The Cloud Storage URI of a KMS encrypted file containing the master key of the KDC database.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.securityConfig.kerberosConfig.keyPasswordURI</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided key. For the self-signed certificate, this password is generated by Dataproc.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.securityConfig.kerberosConfig.keystorePasswordURI</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided keystore. For the self-signed certificate, this password is generated by Dataproc.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.securityConfig.kerberosConfig.keystoreURI</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. The Cloud Storage URI of the keystore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.securityConfig.kerberosConfig.kmsKeyURI</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. The URI of the KMS key used to encrypt sensitive files.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.securityConfig.kerberosConfig.realm</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. The name of the on-cluster Kerberos realm. If not specified, the uppercased domain of hostnames will be the realm.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.securityConfig.kerberosConfig.rootPrincipalPasswordURI</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. The Cloud Storage URI of a KMS encrypted file containing the root principal password.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.securityConfig.kerberosConfig.tgtLifetimeHours</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Optional. The lifetime of the ticket granting ticket, in hours. If not specified, or user specifies 0, then default value 10 will be used.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.securityConfig.kerberosConfig.truststorePasswordURI</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided truststore. For the self-signed certificate, this password is generated by Dataproc.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.securityConfig.kerberosConfig.truststoreURI</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. The Cloud Storage URI of the truststore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.softwareConfig</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. The config settings for cluster software.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.softwareConfig.imageVersion</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. The version of software inside the cluster. It must be one of the supported [Dataproc Versions](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#supported-dataproc-image-versions), such as "1.2" (including a subminor version, such as "1.2.29"), or the ["preview" version](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#other_versions). If unspecified, it defaults to the latest Debian version.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.softwareConfig.optionalComponents</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Optional. The set of components to activate on the cluster.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.softwareConfig.optionalComponents[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.softwareConfig.properties</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">map (key: string, value: string)</code></p>
            <p>{% verbatim %}Optional. The properties to set on daemon config files.

 Property keys are specified in `prefix:property` format, for example
 `core:hadoop.tmp.dir`. The following are supported prefixes
 and their mappings:

 * capacity-scheduler: `capacity-scheduler.xml`
 * core:   `core-site.xml`
 * distcp: `distcp-default.xml`
 * hdfs:   `hdfs-site.xml`
 * hive:   `hive-site.xml`
 * mapred: `mapred-site.xml`
 * pig:    `pig.properties`
 * spark:  `spark-defaults.conf`
 * yarn:   `yarn-site.xml`

 For more information, see [Cluster
 properties](https://cloud.google.com/dataproc/docs/concepts/cluster-properties).{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.tempBucket</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. A Cloud Storage bucket used to store ephemeral cluster and jobs data, such as Spark and MapReduce history files. If you do not specify a temp bucket, Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's temp bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket. The default bucket has a TTL of 90 days, but you can use any TTL (or none) if you specify a bucket (see [Dataproc staging and temp buckets](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)). **This field requires a Cloud Storage bucket name, not a `gs://...` URI to a Cloud Storage bucket.**{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.workerConfig</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. The Compute Engine config settings for the cluster's worker instances.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.workerConfig.accelerators</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (object)</code></p>
            <p>{% verbatim %}Optional. The Compute Engine accelerator configuration for these instances.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.workerConfig.accelerators[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.workerConfig.accelerators[].acceleratorCount</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}The number of the accelerator cards of this type exposed to this instance.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.workerConfig.accelerators[].acceleratorTypeURI</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Full URL, partial URI, or short name of the accelerator type resource to
 expose to this instance. See
 [Compute Engine
 AcceleratorTypes](https://cloud.google.com/compute/docs/reference/v1/acceleratorTypes).

 Examples:

 * `https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone]/acceleratorTypes/nvidia-tesla-t4`
 * `projects/[project_id]/zones/[zone]/acceleratorTypes/nvidia-tesla-t4`
 * `nvidia-tesla-t4`

 **Auto Zone Exception**: If you are using the Dataproc
 [Auto Zone
 Placement](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement)
 feature, you must use the short name of the accelerator type
 resource, for example, `nvidia-tesla-t4`.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.workerConfig.diskConfig</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Disk option config settings.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.workerConfig.diskConfig.bootDiskProvisionedIops</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Optional. Indicates how many IOPS to provision for the disk. This sets the number of I/O operations per second that the disk can handle. Note: This field is only supported if boot_disk_type is hyperdisk-balanced.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.workerConfig.diskConfig.bootDiskProvisionedThroughput</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Optional. Indicates how much throughput to provision for the disk. This sets the number of throughput mb per second that the disk can handle. Values must be greater than or equal to 1. Note: This field is only supported if boot_disk_type is hyperdisk-balanced.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.workerConfig.diskConfig.bootDiskSizeGB</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Optional. Size in GB of the boot disk (default is 500GB).{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.workerConfig.diskConfig.bootDiskType</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. Type of the boot disk (default is "pd-standard"). Valid values: "pd-balanced" (Persistent Disk Balanced Solid State Drive), "pd-ssd" (Persistent Disk Solid State Drive), or "pd-standard" (Persistent Disk Hard Disk Drive). See [Disk types](https://cloud.google.com/compute/docs/disks#disk-types).{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.workerConfig.diskConfig.localSsdInterface</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. Interface type of local SSDs (default is "scsi"). Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile Memory Express). See [local SSD performance](https://cloud.google.com/compute/docs/disks/local-ssd#performance).{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.workerConfig.diskConfig.numLocalSsds</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Optional. Number of attached SSDs, from 0 to 8 (default is 0).
 If SSDs are not attached, the boot disk is used to store runtime logs and
 [HDFS](https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data.
 If one or more SSDs are attached, this runtime bulk
 data is spread across them, and the boot disk contains only basic
 config and installed binaries.

 Note: Local SSD options may vary by machine type and number of vCPUs
 selected.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.workerConfig.imageURI</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. The Compute Engine image resource used for cluster instances.

 The URI can represent an image or image family.

 Image examples:

 * `https://www.googleapis.com/compute/v1/projects/[project_id]/global/images/[image-id]`
 * `projects/[project_id]/global/images/[image-id]`
 * `image-id`

 Image family examples. Dataproc will use the most recent
 image from the family:

 * `https://www.googleapis.com/compute/v1/projects/[project_id]/global/images/family/[custom-image-family-name]`
 * `projects/[project_id]/global/images/family/[custom-image-family-name]`

 If the URI is unspecified, it will be inferred from
 `SoftwareConfig.image_version` or the system default.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.workerConfig.instanceFlexibilityPolicy</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Instance flexibility Policy allowing a mixture of VM shapes and provisioning models.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.workerConfig.instanceFlexibilityPolicy.instanceSelectionList</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (object)</code></p>
            <p>{% verbatim %}Optional. List of instance selection options that the group will use when creating new VMs.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.workerConfig.instanceFlexibilityPolicy.instanceSelectionList[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.workerConfig.instanceFlexibilityPolicy.instanceSelectionList[].machineTypes</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Optional. Full machine-type names, e.g. "n1-standard-16".{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.workerConfig.instanceFlexibilityPolicy.instanceSelectionList[].machineTypes[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.workerConfig.instanceFlexibilityPolicy.instanceSelectionList[].rank</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Optional. Preference of this instance selection. Lower number means higher preference. Dataproc will first try to create a VM based on the machine-type with priority rank and fallback to next rank based on availability. Machine types and instance selections with the same priority have the same preference.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.workerConfig.instanceFlexibilityPolicy.provisioningModelMix</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Defines how the Group selects the provisioning model to ensure required reliability.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.workerConfig.instanceFlexibilityPolicy.provisioningModelMix.standardCapacityBase</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Optional. The base capacity that will always use Standard VMs to avoid risk of more preemption than the minimum capacity you need. Dataproc will create only standard VMs until it reaches standard_capacity_base, then it will start using standard_capacity_percent_above_base to mix Spot with Standard VMs. eg. If 15 instances are requested and standard_capacity_base is 5, Dataproc will create 5 standard VMs and then start mixing spot and standard VMs for remaining 10 instances.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.workerConfig.instanceFlexibilityPolicy.provisioningModelMix.standardCapacityPercentAboveBase</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Optional. The percentage of target capacity that should use Standard VM. The remaining percentage will use Spot VMs. The percentage applies only to the capacity above standard_capacity_base. eg. If 15 instances are requested and standard_capacity_base is 5 and standard_capacity_percent_above_base is 30, Dataproc will create 5 standard VMs and then start mixing spot and standard VMs for remaining 10 instances. The mix will be 30% standard and 70% spot.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.workerConfig.machineTypeURI</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. The Compute Engine machine type used for cluster instances.

 A full URL, partial URI, or short name are valid. Examples:

 * `https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone]/machineTypes/n1-standard-2`
 * `projects/[project_id]/zones/[zone]/machineTypes/n1-standard-2`
 * `n1-standard-2`

 **Auto Zone Exception**: If you are using the Dataproc
 [Auto Zone
 Placement](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement)
 feature, you must use the short name of the machine type
 resource, for example, `n1-standard-2`.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.workerConfig.minCPUPlatform</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. Specifies the minimum cpu platform for the Instance Group. See [Dataproc -> Minimum CPU Platform](https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.workerConfig.minNumInstances</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Optional. The minimum number of primary worker instances to create.
 If `min_num_instances` is set, cluster creation will succeed if
 the number of primary workers created is at least equal to the
 `min_num_instances` number.

 Example: Cluster creation request with `num_instances` = `5` and
 `min_num_instances` = `3`:

 *  If 4 VMs are created and 1 instance fails,
    the failed VM is deleted. The cluster is
    resized to 4 instances and placed in a `RUNNING` state.
 *  If 2 instances are created and 3 instances fail,
    the cluster in placed in an `ERROR` state. The failed VMs
    are not deleted.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.workerConfig.numInstances</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Optional. The number of VM instances in the instance group. For [HA cluster](/dataproc/docs/concepts/configuring-clusters/high-availability) [master_config](#FIELDS.master_config) groups, **must be set to 3**. For standard cluster [master_config](#FIELDS.master_config) groups, **must be set to 1**.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.workerConfig.preemptibility</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. Specifies the preemptibility of the instance group.

 The default value for master and worker groups is
 `NON_PREEMPTIBLE`. This default cannot be changed.

 The default value for secondary instances is
 `PREEMPTIBLE`.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.workerConfig.startupConfig</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Configuration to handle the startup of instances during cluster create and update process.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.config.workerConfig.startupConfig.requiredRegistrationFraction</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">float</code></p>
            <p>{% verbatim %}Optional. The config setting to enable cluster creation/ updation to be successful only after required_registration_fraction of instances are up and running. This configuration is applicable to only secondary workers for now. The cluster will fail if required_registration_fraction of instances are not available. This will include instance creation, agent registration, and service registration (if enabled).{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>placement.managedCluster.labels</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">map (key: string, value: string)</code></p>
            <p>{% verbatim %}Optional. The labels to associate with this cluster.

 Label keys must be between 1 and 63 characters long, and must conform to
 the following PCRE regular expression:
 [\p{Ll}\p{Lo}][\p{Ll}\p{Lo}\p{N}_-]{0,62}

 Label values must be between 1 and 63 characters long, and must conform to
 the following PCRE regular expression: [\p{Ll}\p{Lo}\p{N}_-]{0,63}

 No more than 32 labels can be associated with a given cluster.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>projectRef</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Required.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>projectRef.external</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}The `projectID` field of a project, when not managed by Config Connector.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>projectRef.kind</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}The kind of the Project resource; optional but must be `Project` if provided.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>projectRef.name</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}The `name` field of a `Project` resource.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>projectRef.namespace</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}The `namespace` field of a `Project` resource.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>resourceID</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}The DataprocWorkflowTemplate name. If not given, the metadata.name will be used.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>version</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Optional. Used to perform a consistent read-modify-write.

 This field should be left blank for a `CreateWorkflowTemplate` request. It
 is required for an `UpdateWorkflowTemplate` request, and must match the
 current server version. A typical update template flow would fetch the
 current template with a `GetWorkflowTemplate` request, which will return
 the current template with the `version` field filled in with the
 current server version. The user updates other fields in the template,
 then returns it as part of the `UpdateWorkflowTemplate` request.{% endverbatim %}</p>
        </td>
    </tr>
</tbody>
</table>



### Status
#### Schema
```yaml
conditions:
- lastTransitionTime: string
  message: string
  reason: string
  status: string
  type: string
externalRef: string
observedGeneration: integer
observedState:
  createTime: string
  name: string
  placement:
    managedCluster:
      config:
        endpointConfig:
          httpPorts:
            string: string
        lifecycleConfig:
          idleStartTime: string
        masterConfig:
          instanceFlexibilityPolicy:
            instanceSelectionResults:
            - {}
          instanceNames:
          - string
          instanceReferences:
          - instanceID: string
            instanceName: string
            publicEciesKey: string
            publicKey: string
          isPreemptible: boolean
          managedGroupConfig: {}
  updateTime: string
```

<table class="properties responsive">
<thead>
    <tr>
        <th colspan="2">Fields</th>
    </tr>
</thead>
<tbody>
    <tr>
        <td><code>conditions</code></td>
        <td>
            <p><code class="apitype">list (object)</code></p>
            <p>{% verbatim %}Conditions represent the latest available observations of the object's current state.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>conditions[]</code></td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>conditions[].lastTransitionTime</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Last time the condition transitioned from one status to another.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>conditions[].message</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Human-readable message indicating details about last transition.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>conditions[].reason</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Unique, one-word, CamelCase reason for the condition's last transition.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>conditions[].status</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Status is the status of the condition. Can be True, False, Unknown.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>conditions[].type</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Type is the type of the condition.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>externalRef</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}A unique specifier for the DataprocWorkflowTemplate resource in GCP.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedGeneration</code></td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}ObservedGeneration is the generation of the resource that was most recently observed by the Config Connector controller. If this is equal to metadata.generation, then that means that the current reported status reflects the most recent desired state of the resource.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState</code></td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}ObservedState is the state of the resource as most recently observed in GCP.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.createTime</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Output only. The time template was created.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.name</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Output only. The resource name of the workflow template, as described
 in https://cloud.google.com/apis/design/resource_names.

 * For `projects.regions.workflowTemplates`, the resource name of the
   template has the following format:
   `projects/{project_id}/regions/{region}/workflowTemplates/{template_id}`

 * For `projects.locations.workflowTemplates`, the resource name of the
   template has the following format:
   `projects/{project_id}/locations/{location}/workflowTemplates/{template_id}`{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.placement</code></td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Required. WorkflowTemplate scheduling information.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.placement.managedCluster</code></td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}A cluster that is managed by the workflow.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.placement.managedCluster.config</code></td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Required. The cluster configuration.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.placement.managedCluster.config.endpointConfig</code></td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Port/endpoint configuration for this cluster{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.placement.managedCluster.config.endpointConfig.httpPorts</code></td>
        <td>
            <p><code class="apitype">map (key: string, value: string)</code></p>
            <p>{% verbatim %}Output only. The map of port descriptions to URLs. Will only be populated if enable_http_port_access is true.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.placement.managedCluster.config.lifecycleConfig</code></td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Lifecycle setting for the cluster.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.placement.managedCluster.config.lifecycleConfig.idleStartTime</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Output only. The time when cluster became idle (most recent job finished) and became eligible for deletion due to idleness (see JSON representation of [Timestamp](https://developers.google.com/protocol-buffers/docs/proto3#json)).{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.placement.managedCluster.config.masterConfig</code></td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. The Compute Engine config settings for the cluster's master instance.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.placement.managedCluster.config.masterConfig.instanceFlexibilityPolicy</code></td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Instance flexibility Policy allowing a mixture of VM shapes and provisioning models.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.placement.managedCluster.config.masterConfig.instanceFlexibilityPolicy.instanceSelectionResults</code></td>
        <td>
            <p><code class="apitype">list (object)</code></p>
            <p>{% verbatim %}Output only. A list of instance selection results in the group.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.placement.managedCluster.config.masterConfig.instanceFlexibilityPolicy.instanceSelectionResults[]</code></td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.placement.managedCluster.config.masterConfig.instanceNames</code></td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Output only. The list of instance names. Dataproc derives the names from `cluster_name`, `num_instances`, and the instance group.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.placement.managedCluster.config.masterConfig.instanceNames[]</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.placement.managedCluster.config.masterConfig.instanceReferences</code></td>
        <td>
            <p><code class="apitype">list (object)</code></p>
            <p>{% verbatim %}Output only. List of references to Compute Engine instances.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.placement.managedCluster.config.masterConfig.instanceReferences[]</code></td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.placement.managedCluster.config.masterConfig.instanceReferences[].instanceID</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}The unique identifier of the Compute Engine instance.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.placement.managedCluster.config.masterConfig.instanceReferences[].instanceName</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}The user-friendly name of the Compute Engine instance.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.placement.managedCluster.config.masterConfig.instanceReferences[].publicEciesKey</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}The public ECIES key used for sharing data with this instance.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.placement.managedCluster.config.masterConfig.instanceReferences[].publicKey</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}The public RSA key used for sharing data with this instance.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.placement.managedCluster.config.masterConfig.isPreemptible</code></td>
        <td>
            <p><code class="apitype">boolean</code></p>
            <p>{% verbatim %}Output only. Specifies that this instance group contains preemptible instances.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.placement.managedCluster.config.masterConfig.managedGroupConfig</code></td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Output only. The config for Compute Engine Instance Group Manager that manages this group. This is only used for preemptible instance groups.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.updateTime</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Output only. The time template was last updated.{% endverbatim %}</p>
        </td>
    </tr>
</tbody>
</table>

## Sample YAML(s)

### Typical Use Case
```yaml
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

apiVersion: dataproc.cnrm.cloud.google.com/v1beta1
kind: DataprocWorkflowTemplate
metadata:
  labels:
    label-one: "value-one"
  name: dataprocworkflowtemplate-sample
spec:
  location: "us-central1"
  placement:
    managedCluster:
      clusterName: "test-cluster"
      config:
        autoscalingConfig:
          policyRef:
            name: dataprocworkflowtemplate-dep
        masterConfig:
          diskConfig:
            bootDiskSizeGb: 30
            bootDiskType: pd-standard
          machineType: "n2-standard-8"
          numInstances: 1
        workerConfig:
          numInstances: 2
          machineType: "n2-standard-8"
          diskConfig:
            bootDiskSizeGb: 30
            numLocalSsds: 1
        softwareConfig:
          imageVersion: "2.0.39-debian10"
        gceClusterConfig:
          tags:
          - "foo"
          - "bar"
  jobs:
  - stepId: "someJob"
    sparkJob:
      mainClass: "SomeClass"
  - stepId: "otherJob"
    prerequisiteStepIds:
    - "someJob"
    prestoJob:
      queryFileUri: "someUri"
---
apiVersion: dataproc.cnrm.cloud.google.com/v1beta1
kind: DataprocAutoscalingPolicy
metadata:
  name: dataprocworkflowtemplate-dep
spec:
  location: "us-central1"
  workerConfig:
    maxInstances: 5
  secondaryWorkerConfig:
    maxInstances: 2
  basicAlgorithm:
    yarnConfig:
      gracefulDecommissionTimeout: "30s"
      scaleDownFactor: 0.5
      scaleUpFactor: 1
```


Note: If you have any trouble with instantiating the resource, refer to <a href="/config-connector/docs/troubleshooting">Troubleshoot Config Connector</a>.

{% endblock %}