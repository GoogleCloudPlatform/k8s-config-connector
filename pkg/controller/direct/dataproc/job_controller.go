// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//      http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// +tool:controller
// proto.service: google.cloud.dataproc.v1.JobController
// proto.message: google.cloud.dataproc.v1.Job
// crd.type: DataprocJob
// crd.version: v1alpha1

package dataproc

import (
	"context"
	"fmt"

	dataproc "cloud.google.com/go/dataproc/v2/apiv1"
	pb "cloud.google.com/go/dataproc/v2/apiv1/dataprocpb"
	"google.golang.org/protobuf/proto"
	"google.golang.org/protobuf/types/known/fieldmaskpb"
	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/klog/v2"
	"sigs.k8s.io/controller-runtime/pkg/client"

	krm "github.com/GoogleCloudPlatform/k8s-config-connector/apis/dataproc/v1alpha1"
	refs "github.com/GoogleCloudPlatform/k8s-config-connector/apis/refs/v1beta1"
	"github.com/GoogleCloudPlatform/k8s-config-connector/pkg/config"
	"github.com/GoogleCloudPlatform/k8s-config-connector/pkg/controller/direct"
	"github.com/GoogleCloudPlatform/k8s-config-connector/pkg/controller/direct/directbase"
	"github.com/GoogleCloudPlatform/k8s-config-connector/pkg/controller/direct/registry"
)

const (
	// Dataproc jobs cannot be updated.
	// Ref: https://cloud.google.com/dataproc/docs/reference/rest/v1/projects.regions.jobs/patch
	// "Only the job state and labels are mutable."
	// State is managed by the service, and labels are metadata.
	// We consider the core job configuration immutable.
	dataprocJobErrCannotUpdate = "dataproc jobs cannot be updated"
)

func init() {
	registry.RegisterModel(krm.DataprocJobGVK, NewDataprocJobModel)
}

func NewDataprocJobModel(ctx context.Context, config *config.ControllerConfig) (directbase.Model, error) {
	return &dataprocJobModel{config: *config}, nil
}

var _ directbase.Model = &dataprocJobModel{}

type dataprocJobModel struct {
	config config.ControllerConfig
}

func (m *dataprocJobModel) client(ctx context.Context) (*dataproc.JobControllerClient, error) {
	opts, err := m.config.RESTClientOptions()
	if err != nil {
		return nil, err
	}

	return dataproc.NewJobControllerRESTClient(ctx, opts...)
}

func (m *dataprocJobModel) AdapterForObject(ctx context.Context, reader client.Reader, u *unstructured.Unstructured) (directbase.Adapter, error) {
	obj := &krm.DataprocJob{}
	if err := runtime.DefaultUnstructuredConverter.FromUnstructured(u.Object, &obj); err != nil {
		return nil, fmt.Errorf("error converting to %T: %w", obj, err)
	}

	// The job ID is part of the job placement config in the proto, not the top-level resource ID.
	// The job resource itself uses a UUID generated by the service (`reference.jobId`).
	// We construct an internal ID representation that includes the project and location.
	// When creating, we don't know the service-generated Job ID yet.
	jobID, err := krm.NewJobIdentity(ctx, reader, obj)
	if err != nil {
		return nil, fmt.Errorf("failed to create new Job identity, error: %w", err)
	}

	gcpClient, err := m.client(ctx)
	if err != nil {
		return nil, err
	}

	return &dataprocJobAdapter{
		gcpClient:   gcpClient,
		id:          jobID,
		desired:     obj,
		generatedId: direct.LazyPtr(jobID.ID()),
	}, nil
}

func (m *dataprocJobModel) AdapterForURL(ctx context.Context, url string) (directbase.Adapter, error) {
	// TODO: Support URLs
	return nil, nil
}

var _ directbase.Adapter = &dataprocJobAdapter{}

type dataprocJobAdapter struct {
	gcpClient   *dataproc.JobControllerClient
	id          *krm.JobIdentity // Contains ProjectID, Region, and potentially JobID (after creation or from URL)
	desired     *krm.DataprocJob // Desired state
	actual      *pb.Job          // Actual state fetched from GCP
	generatedId *string
}

func (a *dataprocJobAdapter) Find(ctx context.Context) (bool, error) {
	if a.id.ID() == "" {
		// If JobID is not known (e.g., before creation), we can't GET the job.
		// However, we might try to find it using a list call with a filter if
		// the user provided a stable ID via placement.jobId or resourceID annotation.
		// For now, assume we can only find by service-generated ID.
		// TODO(kcc): Implement List+Filter based on user-provided ID if available.
		klog.V(2).Infof("cannot find dataproc job without a job ID (project %s, region %s)", a.id.Parent().ProjectID, a.id.Parent().Location)
		return false, nil
	}

	req := &pb.GetJobRequest{
		ProjectId: a.id.Parent().ProjectID,
		Region:    a.id.Parent().Location,
		JobId:     direct.ValueOf(a.generatedId),
	}

	klog.V(2).Infof("getting dataproc job %q", a.id)
	actual, err := a.gcpClient.GetJob(ctx, req)
	if err != nil {
		if direct.IsNotFound(err) {
			klog.V(2).Infof("dataproc job %q not found", a.id)
			return false, nil
		}
		return false, fmt.Errorf("getting dataproc job %q: %w", a.id, err)
	}
	klog.V(2).Infof("found dataproc job %q", a.id)

	a.actual = actual

	// Check if the job is terminated (completed, cancelled, errored).
	// Once terminated, it's effectively immutable and shouldn't be reconciled further.
	// We treat it as "found" but signal it shouldn't be updated/deleted by KCC if it's done.
	// KCC deletion should still work if the user deletes the KRM object.
	// if a.actual.Status != nil && (a.actual.Status.State == pb.JobStatus_DONE || a.actual.Status.State == pb.JobStatus_CANCELLED || a.actual.Status.State == pb.JobStatus_ERROR) {
	// 	klog.Infof("Dataproc job %s/%s/%s is already in a terminal state (%s).", a.id.ProjectID, a.id.Region, a.id.JobID, a.actual.Status.State)
	// 	// TODO: How to signal this state back to the controller logic? Maybe via status?
	// }

	return true, nil
}

func (a *dataprocJobAdapter) Create(ctx context.Context, createOp *directbase.CreateOperation) error {
	klog.V(2).Infof("creating dataproc job in project %s region %s", a.id.Parent().ProjectID, a.id.Parent().Location)

	mapCtx := &direct.MapContext{}
	desiredProto := DataprocJobSpec_ToProto(mapCtx, &a.desired.Spec)
	if mapCtx.Err() != nil {
		return mapCtx.Err()
	}
	// We need the full Job message, not just the spec part.
	desired := &pb.Job{
		Placement:              desiredProto.Placement,
		JobUuid:                desiredProto.JobUuid,
		Scheduling:             desiredProto.Scheduling,
		YarnApplications:       desiredProto.YarnApplications,
		DriverSchedulingConfig: desiredProto.DriverSchedulingConfig,
		Done:                   desiredProto.Done,
		Labels:                 desiredProto.Labels,
	}
	// Populate the job type specific field
	if a.desired.Spec.HadoopJob != nil {
		desired.TypeJob = &pb.Job_HadoopJob{HadoopJob: desiredProto.GetHadoopJob()}
	} else if a.desired.Spec.SparkJob != nil {
		desired.TypeJob = &pb.Job_SparkJob{SparkJob: desiredProto.GetSparkJob()}
	} else if a.desired.Spec.PysparkJob != nil {
		desired.TypeJob = &pb.Job_PysparkJob{PysparkJob: desiredProto.GetPysparkJob()}
	} else if a.desired.Spec.HiveJob != nil {
		desired.TypeJob = &pb.Job_HiveJob{HiveJob: desiredProto.GetHiveJob()}
	} else if a.desired.Spec.PigJob != nil {
		desired.TypeJob = &pb.Job_PigJob{PigJob: desiredProto.GetPigJob()}
	} else if a.desired.Spec.SparkRJob != nil {
		desired.TypeJob = &pb.Job_SparkRJob{SparkRJob: desiredProto.GetSparkRJob()}
	} else if a.desired.Spec.SparkSQLJob != nil {
		desired.TypeJob = &pb.Job_SparkSqlJob{SparkSqlJob: desiredProto.GetSparkSqlJob()}
	} else if a.desired.Spec.PrestoJob != nil {
		desired.TypeJob = &pb.Job_PrestoJob{PrestoJob: desiredProto.GetPrestoJob()}
	} else if a.desired.Spec.TrinoJob != nil {
		desired.TypeJob = &pb.Job_TrinoJob{TrinoJob: desiredProto.GetTrinoJob()}
	} else {
		return fmt.Errorf("unknown job type specified")
	}

	// Set the user-specified ID if provided in the spec.
	if desired.GetPlacement() != nil && desired.GetPlacement().GetClusterUuid() != "" {
		// Placement.JobUuid is deprecated. Prefer setting reference.job_id
	} else {
		// If no specific job ID is in the spec, use the KRM resource name/resourceID
		// as the reference ID for submission. This makes jobs findable later if the
		// service-generated UUID is lost.
		if desired.Reference == nil {
			desired.Reference = &pb.JobReference{}
		}
	}

	desired.Reference.JobId = a.id.ID()
	if desired.Labels == nil {
		desired.Labels = make(map[string]string)
	}
	desired.Labels["managed-by-cnrm"] = "true"

	// Ensure placement is non-nil if it wasn't set, as it's often needed implicitly
	// even if empty (e.g., for cluster selectors). The mapping function should handle this.

	req := &pb.SubmitJobRequest{
		ProjectId: a.id.Parent().ProjectID,
		Region:    a.id.Parent().Location,
		Job:       desired,
	}

	submittedJob, err := a.gcpClient.SubmitJob(ctx, req)
	if err != nil {
		return fmt.Errorf("submitting dataproc job in %s/%s: %w", a.id.Parent().ProjectID, a.id.Parent().Location, err)
	}
	klog.V(2).Infof("successfully submitted dataproc job %s/%s, service generated ID: %s", a.id.Parent().ProjectID, a.id.Parent().Location, submittedJob.GetReference().GetJobId())

	a.actual = submittedJob // Store the immediate result as 'actual' state

	// Map the *returned* job state to the KRM status
	status := &krm.DataprocJobStatus{}
	status.ObservedState = DataprocJobObservedState_FromProto(mapCtx, submittedJob)
	if mapCtx.Err() != nil {
		return mapCtx.Err()
	}

	// ExternalRef needs the service-generated ID
	status.ExternalRef = direct.PtrTo(fmt.Sprintf("projects/%s/regions/%s/jobs/%s", a.id.Parent().ProjectID, a.id.Parent().Location, a.id.ID()))
	a.generatedId = direct.LazyPtr(submittedJob.JobUuid)
	if mapCtx.Err() != nil {
		return mapCtx.Err()
	}

	return createOp.UpdateStatus(ctx, status, nil)
}

func (a *dataprocJobAdapter) Update(ctx context.Context, updateOp *directbase.UpdateOperation) error {
	// Dataproc jobs are generally immutable after submission, except for labels.
	// Ref: https://cloud.google.com/dataproc/docs/reference/rest/v1/projects.regions.jobs/patch
	// We prevent updates to avoid unexpected behavior or errors.
	klog.Warningf("dataproc job %q: updates are not supported", a.id)

	// Check if only labels changed
	onlyLabelsChanged := true
	if a.actual == nil {
		// Should not happen in Update path, but defensively return error
		return fmt.Errorf("actual state is nil during update for job %q", a.id)
	}

	mapCtx := &direct.MapContext{}
	desiredProto := DataprocJobSpec_ToProto(mapCtx, &a.desired.Spec)
	if mapCtx.Err() != nil {
		return mapCtx.Err()
	}
	// We need the full Job message, not just the spec part.
	desired := &pb.Job{
		Placement:              desiredProto.Placement,
		JobUuid:                desiredProto.JobUuid,
		Scheduling:             desiredProto.Scheduling,
		YarnApplications:       desiredProto.YarnApplications,
		DriverSchedulingConfig: desiredProto.DriverSchedulingConfig,
		Done:                   desiredProto.Done,
		Labels:                 desiredProto.Labels,
	}
	// Populate the job type specific field
	if a.desired.Spec.HadoopJob != nil {
		desired.TypeJob = &pb.Job_HadoopJob{HadoopJob: desiredProto.GetHadoopJob()}
	} else if a.desired.Spec.SparkJob != nil {
		desired.TypeJob = &pb.Job_SparkJob{SparkJob: desiredProto.GetSparkJob()}
	} else if a.desired.Spec.PysparkJob != nil {
		desired.TypeJob = &pb.Job_PysparkJob{PysparkJob: desiredProto.GetPysparkJob()}
	} else if a.desired.Spec.HiveJob != nil {
		desired.TypeJob = &pb.Job_HiveJob{HiveJob: desiredProto.GetHiveJob()}
	} else if a.desired.Spec.PigJob != nil {
		desired.TypeJob = &pb.Job_PigJob{PigJob: desiredProto.GetPigJob()}
	} else if a.desired.Spec.SparkRJob != nil {
		desired.TypeJob = &pb.Job_SparkRJob{SparkRJob: desiredProto.GetSparkRJob()}
	} else if a.desired.Spec.SparkSQLJob != nil {
		desired.TypeJob = &pb.Job_SparkSqlJob{SparkSqlJob: desiredProto.GetSparkSqlJob()}
	} else if a.desired.Spec.PrestoJob != nil {
		desired.TypeJob = &pb.Job_PrestoJob{PrestoJob: desiredProto.GetPrestoJob()}
	} else if a.desired.Spec.TrinoJob != nil {
		desired.TypeJob = &pb.Job_TrinoJob{TrinoJob: desiredProto.GetTrinoJob()}
	} else {
		return fmt.Errorf("unknown job type specified")
	}

	// Create copies for comparison, normalizing potential output-only fields
	desiredComparable := proto.Clone(desired).(*pb.Job)
	actualComparable := proto.Clone(a.actual).(*pb.Job)

	// Clear fields that are not part of the spec or are output-only before comparison
	clearOutputOnlyJobFields(desiredComparable)
	clearOutputOnlyJobFields(actualComparable)

	// Compare everything except labels
	desiredLabels := desiredComparable.Labels
	actualLabels := actualComparable.Labels
	desiredComparable.Labels = nil
	actualComparable.Labels = nil

	if !proto.Equal(desiredComparable, actualComparable) {
		onlyLabelsChanged = false
	}

	// Restore labels for potential label update
	desiredComparable.Labels = desiredLabels
	actualComparable.Labels = actualLabels

	if !onlyLabelsChanged {
		klog.Warningf("Dataproc Job can only update labels.")
		// We still need to update the status with the observed state from Find()
		status := &krm.DataprocJobStatus{}
		mapCtx := &direct.MapContext{}
		status.ObservedState = DataprocJobObservedState_FromProto(mapCtx, a.actual)
		status.ExternalRef = direct.PtrTo(fmt.Sprintf("projects/%s/regions/%s/jobs/%s", a.id.Parent().ProjectID, a.id.Parent().Location, a.id.ID()))
		if mapCtx.Err() != nil {
			return mapCtx.Err()
		}
		return updateOp.UpdateStatus(ctx, status, nil)
	}

	// If only labels changed, proceed with label update
	if !MapsEqual(a.desired.Labels, a.actual.Labels) {
		klog.V(2).Infof("updating labels for dataproc job %q", a.id)
		req := &pb.UpdateJobRequest{
			ProjectId: a.id.Parent().ProjectID,
			Region:    a.id.Parent().Location,
			JobId:     a.id.ID(),
			Job: &pb.Job{
				Labels: a.desired.Labels,
			},
			UpdateMask: &fieldmaskpb.FieldMask{Paths: []string{"labels"}},
		}
		updatedJob, err := a.gcpClient.UpdateJob(ctx, req)
		if err != nil {
			return fmt.Errorf("updating labels for dataproc job %q: %w", a.id, err)
		}
		klog.V(2).Infof("successfully updated labels for dataproc job %q", a.id)
		a.actual = updatedJob // Update actual state

		// Map the updated job state to the KRM status
		mapCtx := &direct.MapContext{}
		status := &krm.DataprocJobStatus{}
		status.ObservedState = DataprocJobObservedState_FromProto(mapCtx, updatedJob)
		status.ExternalRef = direct.PtrTo(fmt.Sprintf("projects/%s/regions/%s/jobs/%s", a.id.Parent().ProjectID, a.id.Parent().Location, a.id.ID()))
		if mapCtx.Err() != nil {
			return mapCtx.Err()
		}
		return updateOp.UpdateStatus(ctx, status, nil)
	}

	klog.V(2).Infof("no update needed for dataproc job %q", a.id)
	return nil // No changes detected
}

// clearOutputOnlyJobFields removes fields from a Job proto that are output-only
// or managed by the service, so they don't interfere with comparisons.
func clearOutputOnlyJobFields(job *pb.Job) {
	if job == nil {
		return
	}
	job.Status = nil
	job.StatusHistory = nil
	job.DriverOutputResourceUri = ""
	job.DriverControlFilesUri = ""
	job.JobUuid = ""           // Service-generated UUID
	job.Reference = nil        // Contains service-generated ID
	job.YarnApplications = nil // Runtime info
	job.Done = false           // Runtime status
	// Placement might contain output info like cluster UUID if not specified by name
	if job.Placement != nil {
		job.Placement.ClusterUuid = ""
	}
}

func (a *dataprocJobAdapter) Export(ctx context.Context) (*unstructured.Unstructured, error) {
	if a.actual == nil {
		return nil, fmt.Errorf("actual state is nil, cannot export")
	}

	mapCtx := &direct.MapContext{}
	spec := DataprocJobSpec_FromProto(mapCtx, a.actual)
	if mapCtx.Err() != nil {
		return nil, mapCtx.Err()
	}

	obj := &krm.DataprocJob{
		Spec: *spec,
	}
	obj.Spec.ProjectRef = &refs.ProjectRef{External: a.id.Parent().ProjectID}
	obj.Spec.Location = a.id.Parent().Location

	// Set the correct job type wrapper in the spec
	switch jobType := a.actual.TypeJob.(type) {
	case *pb.Job_HadoopJob:
		obj.Spec.HadoopJob = spec.HadoopJob
	case *pb.Job_SparkJob:
		obj.Spec.SparkJob = spec.SparkJob
	case *pb.Job_PysparkJob:
		obj.Spec.PysparkJob = spec.PysparkJob
	case *pb.Job_HiveJob:
		obj.Spec.HiveJob = spec.HiveJob
	case *pb.Job_PigJob:
		obj.Spec.PigJob = spec.PigJob
	case *pb.Job_SparkRJob:
		obj.Spec.SparkRJob = spec.SparkRJob
	case *pb.Job_SparkSqlJob:
		obj.Spec.SparkSQLJob = spec.SparkSQLJob
	case *pb.Job_PrestoJob:
		obj.Spec.PrestoJob = spec.PrestoJob
	case *pb.Job_TrinoJob:
		obj.Spec.TrinoJob = spec.TrinoJob
	default:
		// This should ideally not happen if the job was fetched correctly
		klog.Warningf("unknown job type found during export: %T", jobType)
	}
	obj.Status = krm.DataprocJobStatus{}
	obj.Status.ObservedState = DataprocJobObservedState_FromProto(mapCtx, a.actual)

	uObj, err := runtime.DefaultUnstructuredConverter.ToUnstructured(obj)
	if err != nil {
		return nil, fmt.Errorf("converting DataprocJob to unstructured: %w", err)
	}

	u := &unstructured.Unstructured{Object: uObj}
	u.SetGroupVersionKind(krm.DataprocJobGVK)

	// Use the original reference job ID (if provided) or the service-generated ID as the name.
	// Fallback to a generated name if needed, though unlikely.
	name := a.actual.GetReference().GetJobId()
	if name == "" {
		// This should not happen for a fetched job
		name = "dataproc-job-" + a.id.ID()
	}
	u.SetName(name)

	klog.V(2).Infof("exported dataproc job %q as KRM object", a.id)
	return u, nil
}

func (a *dataprocJobAdapter) Delete(ctx context.Context, deleteOp *directbase.DeleteOperation) (bool, error) {
	// Dataproc jobs can be cancelled, which stops them. Deleting removes the record.
	// We will perform DeleteJob. If the job is terminal, Delete might be idempotent or unnecessary.
	// If the job is active, cancellation might be preferred, but KCC delete means remove the resource.
	if a.id.ID() == "" {
		// Cannot delete if we don't have the ID. This might mean the job was never created successfully.
		klog.Warningf("cannot delete dataproc job in %s/%s, job ID unknown", a.id.Parent().ProjectID, a.id.Parent().Location)
		return true, nil // Assume already gone
	}

	// Check if the job is already in a terminal state. If so, Delete might already be a no-op or unnecessary.
	// However, the API might still allow deleting the record. We proceed with delete regardless.
	if a.actual != nil && a.actual.Status != nil {
		state := a.actual.Status.State
		if state == pb.JobStatus_DONE || state == pb.JobStatus_CANCELLED || state == pb.JobStatus_ERROR {
			klog.V(2).Infof("dataproc job %q is already in terminal state %s, proceeding with deletion", a.id, state)
		}
	}

	klog.V(2).Infof("deleting dataproc job %q", a.id)
	req := &pb.DeleteJobRequest{
		ProjectId: a.id.Parent().ProjectID,
		Region:    a.id.Parent().Location,
		JobId:     direct.ValueOf(a.generatedId),
	}

	err := a.gcpClient.DeleteJob(ctx, req)
	if err != nil {
		if direct.IsNotFound(err) {
			klog.V(2).Infof("dataproc job %q already deleted", a.id)
			return true, nil // Already deleted
		}
		// Check for specific errors, e.g., if cancellation is needed first?
		// The API doc doesn't specify preconditions like needing to cancel first.
		return false, fmt.Errorf("deleting dataproc job %q: %w", a.id, err)
	}

	klog.V(2).Infof("successfully deleted dataproc job %q", a.id)
	return true, nil
}
