// Copyright 2025 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//    http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// ----------------------------------------------------------------------------
//
//     ***     AUTO GENERATED CODE    ***    AUTO GENERATED CODE     ***
//
// ----------------------------------------------------------------------------
//
//     This file is automatically generated by Config Connector and manual
//     changes will be clobbered when the file is regenerated.
//
// ----------------------------------------------------------------------------

// *** DISCLAIMER ***
// Config Connector's go-client for CRDs is currently in ALPHA, which means
// that future versions of the go-client may include breaking changes.
// Please try it out and give us feedback!

package v1alpha1

import (
	"github.com/GoogleCloudPlatform/k8s-config-connector/pkg/clients/generated/apis/k8s/v1alpha1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

type BatchAutotuningConfig struct {
	/* Optional. Scenarios for which tunings are applied. */
	// +optional
	Scenarios []string `json:"scenarios,omitempty"`
}

type BatchEnvironmentConfig struct {
	/* Optional. Execution configuration for a workload. */
	// +optional
	ExecutionConfig *BatchExecutionConfig `json:"executionConfig,omitempty"`

	/* Optional. Peripherals configuration that workload has access to. */
	// +optional
	PeripheralsConfig *BatchPeripheralsConfig `json:"peripheralsConfig,omitempty"`
}

type BatchExecutionConfig struct {
	/* Optional. Applies to sessions only. The duration to keep the session alive while it's idling. Exceeding this threshold causes the session to terminate. This field cannot be set on a batch workload. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of [Duration](https://developers.google.com/protocol-buffers/docs/proto3#json)). Defaults to 1 hour if not set. If both `ttl` and `idle_ttl` are specified for an interactive session, the conditions are treated as `OR` conditions: the workload will be terminated when it has been idle for `idle_ttl` or when `ttl` has been exceeded, whichever occurs first. */
	// +optional
	IdleTTL *string `json:"idleTTL,omitempty"`

	/* Optional. The Cloud KMS key to use for encryption. */
	// +optional
	KmsKeyRef *v1alpha1.ResourceRef `json:"kmsKeyRef,omitempty"`

	/* Optional. Tags used for network traffic control. */
	// +optional
	NetworkTags []string `json:"networkTags,omitempty"`

	/* Optional. Network URI to connect workload to. */
	// +optional
	NetworkURI *string `json:"networkURI,omitempty"`

	/* Optional. Service account that used to execute workload. */
	// +optional
	ServiceAccountRef *v1alpha1.ResourceRef `json:"serviceAccountRef,omitempty"`

	/* Optional. A Cloud Storage bucket used to stage workload dependencies, config files, and store workload output and other ephemeral data, such as Spark history files. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location according to the region where your workload is running, and then create and manage project-level, per-location staging and temporary buckets. **This field requires a Cloud Storage bucket name, not a `gs://...` URI to a Cloud Storage bucket.** */
	// +optional
	StagingBucketRef *v1alpha1.ResourceRef `json:"stagingBucketRef,omitempty"`

	/* Optional. Subnetwork URI to connect workload to. */
	// +optional
	SubnetworkURI *string `json:"subnetworkURI,omitempty"`

	/* Optional. The duration after which the workload will be terminated, specified as the JSON representation for [Duration](https://protobuf.dev/programming-guides/proto3/#json). When the workload exceeds this duration, it will be unconditionally terminated without waiting for ongoing work to finish. If `ttl` is not specified for a batch workload, the workload will be allowed to run until it exits naturally (or run forever without exiting). If `ttl` is not specified for an interactive session, it defaults to 24 hours. If `ttl` is not specified for a batch that uses 2.1+ runtime version, it defaults to 4 hours. Minimum value is 10 minutes; maximum value is 14 days. If both `ttl` and `idle_ttl` are specified (for an interactive session), the conditions are treated as `OR` conditions: the workload will be terminated when it has been idle for `idle_ttl` or when `ttl` has been exceeded, whichever occurs first. */
	// +optional
	Ttl *string `json:"ttl,omitempty"`
}

type BatchPeripheralsConfig struct {
	/* Optional. Resource name of an existing Dataproc Metastore service.

	Example:

	* `projects/[project_id]/locations/[region]/services/[service_id]` */
	// +optional
	MetastoreService *string `json:"metastoreService,omitempty"`

	/* Optional. The Spark History Server configuration for the workload. */
	// +optional
	SparkHistoryServerConfig *BatchSparkHistoryServerConfig `json:"sparkHistoryServerConfig,omitempty"`
}

type BatchPypiRepositoryConfig struct {
	/* Optional. PyPi repository address */
	// +optional
	PypiRepository *string `json:"pypiRepository,omitempty"`
}

type BatchPysparkBatch struct {
	/* Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: `.jar`, `.tar`, `.tar.gz`, `.tgz`, and `.zip`. */
	// +optional
	ArchiveURIs []string `json:"archiveURIs,omitempty"`

	/* Optional. The arguments to pass to the driver. Do not include arguments that can be set as batch properties, such as `--conf`, since a collision can occur that causes an incorrect batch submission. */
	// +optional
	Args []string `json:"args,omitempty"`

	/* Optional. HCFS URIs of files to be placed in the working directory of each executor. */
	// +optional
	FileURIs []string `json:"fileURIs,omitempty"`

	/* Optional. HCFS URIs of jar files to add to the classpath of the Spark driver and tasks. */
	// +optional
	JarFileURIs []string `json:"jarFileURIs,omitempty"`

	/* Required. The HCFS URI of the main Python file to use as the Spark driver. Must be a .py file. */
	// +optional
	MainPythonFileURI *string `json:"mainPythonFileURI,omitempty"`

	/* Optional. HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: `.py`, `.egg`, and `.zip`. */
	// +optional
	PythonFileURIs []string `json:"pythonFileURIs,omitempty"`
}

type BatchRepositoryConfig struct {
	/* Optional. Configuration for PyPi repository. */
	// +optional
	PypiRepositoryConfig *BatchPypiRepositoryConfig `json:"pypiRepositoryConfig,omitempty"`
}

type BatchRuntimeConfig struct {
	/* Optional. Autotuning configuration of the workload. */
	// +optional
	AutotuningConfig *BatchAutotuningConfig `json:"autotuningConfig,omitempty"`

	/* Optional. Cohort identifier. Identifies families of the workloads having the same shape, e.g. daily ETL jobs. */
	// +optional
	Cohort *string `json:"cohort,omitempty"`

	/* Optional. Optional custom container image for the job runtime environment. If not specified, a default container image will be used. */
	// +optional
	ContainerImage *string `json:"containerImage,omitempty"`

	/* Optional. A mapping of property names to values, which are used to configure workload execution. */
	// +optional
	Properties map[string]string `json:"properties,omitempty"`

	/* Optional. Dependency repository configuration. */
	// +optional
	RepositoryConfig *BatchRepositoryConfig `json:"repositoryConfig,omitempty"`

	/* Optional. Version of the batch runtime. */
	// +optional
	Version *string `json:"version,omitempty"`
}

type BatchSparkBatch struct {
	/* Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: `.jar`, `.tar`, `.tar.gz`, `.tgz`, and `.zip`. */
	// +optional
	ArchiveURIs []string `json:"archiveURIs,omitempty"`

	/* Optional. The arguments to pass to the driver. Do not include arguments that can be set as batch properties, such as `--conf`, since a collision can occur that causes an incorrect batch submission. */
	// +optional
	Args []string `json:"args,omitempty"`

	/* Optional. HCFS URIs of files to be placed in the working directory of each executor. */
	// +optional
	FileURIs []string `json:"fileURIs,omitempty"`

	/* Optional. HCFS URIs of jar files to add to the classpath of the Spark driver and tasks. */
	// +optional
	JarFileURIs []string `json:"jarFileURIs,omitempty"`

	/* Optional. The name of the driver main class. The jar file that contains the class must be in the classpath or specified in `jar_file_uris`. */
	// +optional
	MainClass *string `json:"mainClass,omitempty"`

	/* Optional. The HCFS URI of the jar file that contains the main class. */
	// +optional
	MainJarFileURI *string `json:"mainJarFileURI,omitempty"`
}

type BatchSparkHistoryServerConfig struct {
	/* Optional. Resource name of an existing Dataproc Cluster to act as a Spark
	History Server for the workload.

	Example:

	* `projects/[project_id]/regions/[region]/clusters/[cluster_name]` */
	// +optional
	DataprocClusterRef *v1alpha1.ResourceRef `json:"dataprocClusterRef,omitempty"`
}

type BatchSparkRBatch struct {
	/* Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: `.jar`, `.tar`, `.tar.gz`, `.tgz`, and `.zip`. */
	// +optional
	ArchiveURIs []string `json:"archiveURIs,omitempty"`

	/* Optional. The arguments to pass to the Spark driver. Do not include arguments that can be set as batch properties, such as `--conf`, since a collision can occur that causes an incorrect batch submission. */
	// +optional
	Args []string `json:"args,omitempty"`

	/* Optional. HCFS URIs of files to be placed in the working directory of each executor. */
	// +optional
	FileURIs []string `json:"fileURIs,omitempty"`

	/* Required. The HCFS URI of the main R file to use as the driver. Must be a `.R` or `.r` file. */
	// +optional
	MainRFileURI *string `json:"mainRFileURI,omitempty"`
}

type BatchSparkSQLBatch struct {
	/* Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH. */
	// +optional
	JarFileURIs []string `json:"jarFileURIs,omitempty"`

	/* Required. The HCFS URI of the script that contains Spark SQL queries to execute. */
	// +optional
	QueryFileURI *string `json:"queryFileURI,omitempty"`

	/* Optional. Mapping of query variable names to values (equivalent to the Spark SQL command: `SET name="value";`). */
	// +optional
	QueryVariables map[string]string `json:"queryVariables,omitempty"`
}

type DataprocBatchSpec struct {
	/* Optional. Environment configuration for the batch execution. */
	// +optional
	EnvironmentConfig *BatchEnvironmentConfig `json:"environmentConfig,omitempty"`

	/* Optional. The labels to associate with this batch. Label **keys** must contain 1 to 63 characters, and must conform to [RFC 1035](https://www.ietf.org/rfc/rfc1035.txt). Label **values** may be empty, but, if present, must contain 1 to 63 characters, and must conform to [RFC 1035](https://www.ietf.org/rfc/rfc1035.txt). No more than 32 labels can be associated with a batch. */
	// +optional
	Labels map[string]string `json:"labels,omitempty"`

	/* Required. */
	// +optional
	Location *string `json:"location,omitempty"`

	/* Required. */
	// +optional
	ProjectRef *v1alpha1.ResourceRef `json:"projectRef,omitempty"`

	/* Optional. PySpark batch config. */
	// +optional
	PysparkBatch *BatchPysparkBatch `json:"pysparkBatch,omitempty"`

	/* The DataprocBatch name. If not given, the metadata.name will be used. */
	// +optional
	ResourceID *string `json:"resourceID,omitempty"`

	/* Optional. Runtime configuration for the batch execution. */
	// +optional
	RuntimeConfig *BatchRuntimeConfig `json:"runtimeConfig,omitempty"`

	/* Optional. Spark batch config. */
	// +optional
	SparkBatch *BatchSparkBatch `json:"sparkBatch,omitempty"`

	/* Optional. SparkR batch config. */
	// +optional
	SparkRBatch *BatchSparkRBatch `json:"sparkRBatch,omitempty"`

	/* Optional. SparkSql batch config. */
	// +optional
	SparkSQLBatch *BatchSparkSQLBatch `json:"sparkSQLBatch,omitempty"`
}

type BatchApproximateUsageStatus struct {
	/* Optional. Accelerator type being used, if any */
	// +optional
	AcceleratorType *string `json:"acceleratorType,omitempty"`

	/* Optional. Accelerator usage in (`milliAccelerator` x `seconds`) (see [Dataproc Serverless pricing] (https://cloud.google.com/dataproc-serverless/pricing)). */
	// +optional
	MilliAcceleratorSeconds *int64 `json:"milliAcceleratorSeconds,omitempty"`

	/* Optional. DCU (Dataproc Compute Units) usage in (`milliDCU` x `seconds`) (see [Dataproc Serverless pricing] (https://cloud.google.com/dataproc-serverless/pricing)). */
	// +optional
	MilliDcuSeconds *int64 `json:"milliDcuSeconds,omitempty"`

	/* Optional. Shuffle storage usage in (`GB` x `seconds`) (see [Dataproc Serverless pricing] (https://cloud.google.com/dataproc-serverless/pricing)). */
	// +optional
	ShuffleStorageGBSeconds *int64 `json:"shuffleStorageGBSeconds,omitempty"`
}

type BatchCurrentUsageStatus struct {
	/* Optional. Accelerator type being used, if any */
	// +optional
	AcceleratorType *string `json:"acceleratorType,omitempty"`

	/* Optional. Milli (one-thousandth) accelerator. (see [Dataproc Serverless pricing] (https://cloud.google.com/dataproc-serverless/pricing)) */
	// +optional
	MilliAccelerator *int64 `json:"milliAccelerator,omitempty"`

	/* Optional. Milli (one-thousandth) Dataproc Compute Units (DCUs) (see [Dataproc Serverless pricing] (https://cloud.google.com/dataproc-serverless/pricing)). */
	// +optional
	MilliDcu *int64 `json:"milliDcu,omitempty"`

	/* Optional. Milli (one-thousandth) Dataproc Compute Units (DCUs) charged at premium tier (see [Dataproc Serverless pricing] (https://cloud.google.com/dataproc-serverless/pricing)). */
	// +optional
	MilliDcuPremium *int64 `json:"milliDcuPremium,omitempty"`

	/* Optional. Shuffle Storage in gigabytes (GB). (see [Dataproc Serverless pricing] (https://cloud.google.com/dataproc-serverless/pricing)) */
	// +optional
	ShuffleStorageGB *int64 `json:"shuffleStorageGB,omitempty"`

	/* Optional. Shuffle Storage in gigabytes (GB) charged at premium tier. (see [Dataproc Serverless pricing] (https://cloud.google.com/dataproc-serverless/pricing)) */
	// +optional
	ShuffleStorageGBPremium *int64 `json:"shuffleStorageGBPremium,omitempty"`

	/* Optional. The timestamp of the usage snapshot. */
	// +optional
	SnapshotTime *string `json:"snapshotTime,omitempty"`
}

type BatchObservedStateStatus struct {
	/* Output only. The time when the batch was created. */
	// +optional
	CreateTime *string `json:"createTime,omitempty"`

	/* Output only. The email address of the user who created the batch. */
	// +optional
	Creator *string `json:"creator,omitempty"`

	/* Output only. The resource name of the operation associated with this batch. */
	// +optional
	Operation *string `json:"operation,omitempty"`

	/* Output only. Runtime information about batch execution. */
	// +optional
	RuntimeInfo *BatchRuntimeInfoStatus `json:"runtimeInfo,omitempty"`

	/* Output only. The state of the batch. */
	// +optional
	State *string `json:"state,omitempty"`

	/* Output only. Historical state information for the batch. */
	// +optional
	StateHistory []BatchStateHistoryStatus `json:"stateHistory,omitempty"`

	/* Output only. Batch state details, such as a failure description if the state is `FAILED`. */
	// +optional
	StateMessage *string `json:"stateMessage,omitempty"`

	/* Output only. The time when the batch entered a current state. */
	// +optional
	StateTime *string `json:"stateTime,omitempty"`

	/* Output only. A batch UUID (Unique Universal Identifier). The service generates this value when it creates the batch. */
	// +optional
	Uuid *string `json:"uuid,omitempty"`
}

type BatchRuntimeInfoStatus struct {
	/* Output only. Approximate workload resource usage, calculated when
	the workload completes (see [Dataproc Serverless pricing]
	(https://cloud.google.com/dataproc-serverless/pricing)).

	**Note:** This metric calculation may change in the future, for
	example, to capture cumulative workload resource
	consumption during workload execution (see the
	[Dataproc Serverless release notes]
	(https://cloud.google.com/dataproc-serverless/docs/release-notes)
	for announcements, changes, fixes
	and other Dataproc developments). */
	// +optional
	ApproximateUsage *BatchApproximateUsageStatus `json:"approximateUsage,omitempty"`

	/* Output only. Snapshot of current workload resource usage. */
	// +optional
	CurrentUsage *BatchCurrentUsageStatus `json:"currentUsage,omitempty"`

	/* Output only. A URI pointing to the location of the diagnostics tarball. */
	// +optional
	DiagnosticOutputURI *string `json:"diagnosticOutputURI,omitempty"`

	/* Output only. Map of remote access endpoints (such as web interfaces and APIs) to their URIs. */
	// +optional
	Endpoints map[string]string `json:"endpoints,omitempty"`

	/* Output only. A URI pointing to the location of the stdout and stderr of the workload. */
	// +optional
	OutputURI *string `json:"outputURI,omitempty"`
}

type BatchStateHistoryStatus struct {
	/* Output only. The state of the batch at this point in history. */
	// +optional
	State *string `json:"state,omitempty"`

	/* Output only. Details about the state at this point in history. */
	// +optional
	StateMessage *string `json:"stateMessage,omitempty"`

	/* Output only. The time when the batch entered the historical state. */
	// +optional
	StateStartTime *string `json:"stateStartTime,omitempty"`
}

type DataprocBatchStatus struct {
	/* Conditions represent the latest available observations of the
	   DataprocBatch's current state. */
	Conditions []v1alpha1.Condition `json:"conditions,omitempty"`
	/* A unique specifier for the DataprocBatch resource in GCP. */
	// +optional
	ExternalRef *string `json:"externalRef,omitempty"`

	/* ObservedGeneration is the generation of the resource that was most recently observed by the Config Connector controller. If this is equal to metadata.generation, then that means that the current reported status reflects the most recent desired state of the resource. */
	// +optional
	ObservedGeneration *int64 `json:"observedGeneration,omitempty"`

	/* ObservedState is the state of the resource as most recently observed in GCP. */
	// +optional
	ObservedState *BatchObservedStateStatus `json:"observedState,omitempty"`
}

// +genclient
// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object
// +kubebuilder:resource:categories=gcp,shortName=gcpdataprocbatch;gcpdataprocbatches
// +kubebuilder:subresource:status
// +kubebuilder:metadata:labels="cnrm.cloud.google.com/managed-by-kcc=true";"cnrm.cloud.google.com/system=true"
// +kubebuilder:printcolumn:name="Age",JSONPath=".metadata.creationTimestamp",type="date"
// +kubebuilder:printcolumn:name="Ready",JSONPath=".status.conditions[?(@.type=='Ready')].status",type="string",description="When 'True', the most recent reconcile of the resource succeeded"
// +kubebuilder:printcolumn:name="Status",JSONPath=".status.conditions[?(@.type=='Ready')].reason",type="string",description="The reason for the value in 'Ready'"
// +kubebuilder:printcolumn:name="Status Age",JSONPath=".status.conditions[?(@.type=='Ready')].lastTransitionTime",type="date",description="The last transition time for the value in 'Status'"

// DataprocBatch is the Schema for the dataproc API
// +k8s:openapi-gen=true
type DataprocBatch struct {
	metav1.TypeMeta   `json:",inline"`
	metav1.ObjectMeta `json:"metadata,omitempty"`

	Spec   DataprocBatchSpec   `json:"spec,omitempty"`
	Status DataprocBatchStatus `json:"status,omitempty"`
}

// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object

// DataprocBatchList contains a list of DataprocBatch
type DataprocBatchList struct {
	metav1.TypeMeta `json:",inline"`
	metav1.ListMeta `json:"metadata,omitempty"`
	Items           []DataprocBatch `json:"items"`
}

func init() {
	SchemeBuilder.Register(&DataprocBatch{}, &DataprocBatchList{})
}
